%
% Tesi D.S.I. - modello preso da
% Stanford University PhD thesis style -- modifications to the report style
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                         %
%			TESI DOTTORATO                                                   %
%			______________                                                   %
%                                                                         %
%			AUTORE: Elena Pagani                                             %
%                                                                         %
%			Ultima revisione: 7.X.1998                                       %
%                                                                         %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
\documentclass[12pt]{report}
   %\renewcommand{\baselinestretch}{1.5}      % interline spacing
%
% \includeonly{}
%
%			PREAMBOLO
%
\usepackage[a4paper]{geometry}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{url}
\usepackage{wasysym}
\usepackage{hyperref}
\usepackage{adjustbox}
\usepackage{epsfig}
\usepackage[italian]{babel}
% per le accentate
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{tesi}
\usepackage[sorting=none]{biblatex}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{algpseudocode}
\usepackage[detect-all]{siunitx} 


\makeatletter
\renewcommand{\ALG@name}{Algoritmo}
\makeatother

\interfootnotelinepenalty=10000 %% Completely prevent breaking of footnotes
%
\newtheorem{myteor}{Teorema}[section]
%
\theoremstyle{definition}
\newtheorem{exmp}{Esempio}[section]
\setlength\parindent{0pt}
%
\newenvironment{teor}{\begin{myteor}\sl}{\end{myteor}}
\bibliography{references.bib}

%
%
%			TITOLO
%
\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\let\emptyset\varnothing
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1pt\hskip\tabcolsep}}
\makeatother
\begin{document}
\title{Rilevazione di fake news \\
basata sull'induzione di insiemi fuzzy}
\author{Giovanni LAGANÀ}
\dept{Corso di Laurea Magistrale in Informatica}
\anno{2019-2020}
\matricola{928792}
\relatore{Prof. Dario MALCHIODI}
\correlatore{Prof. Alfio FERRARA}
%
%        \submitdate{month year in which submitted to GPO}
%		- date LaTeX'd if omitted
%	\copyrightyear{year degree conferred (next year if submitted in Dec.)}
%		- year LaTeX'd (or next year, in December) if omitted
%	\copyrighttrue or \copyrightfalse
%		- produce or don't produce a copyright page (false by default)
%	\figurespagetrue or \figurespagefalse
%		- produce or don't produce a List of Figures page
%		  (false by default)
%	\tablespagetrue or \tablespagefalse
%		- produce or don't produce a List of Tables page
%		  (false by default)
%
%			DEDICA
%
\beforepreface
        {\hfill \Large {\sl \begin{flushright} Ai miei obiettivi, in attesa di essere raggiunti.         
\end{flushright}         }}
%
%			PREFAZIONE
%
%\prefacesection{Prefazione}
%hkjafgyruet.
%
%%
%%
%%			ORGANIZZAZIONE
%\section*{Organizzazione della tesi}
%\label{organizzazione}
%La tesi \`e organizzata come segue:
%\begin{itemize}
%\item nel Capitolo 1 ....
%\end{itemize}
%
\afterpreface

%
%
%			CAPITOLO 1: 

\chapter*{Introduzione}
\addcontentsline{toc}{chapter}{Introduzione} \markboth{Introduzione}{} 
\onehalfspacing
Con l'avvento della digitalizzazione il settore del giornalismo ha goduto di una forte espansione, causata dalla creazione e diffusione di nuovi mezzi di comunicazione. Di conseguenza, le informazioni diventano sempre più accessibili alle persone in tempi sempre più rapidi, soprattutto grazie agli strumenti di condivisione nati con i social network.
Da una parte, il progresso tecnologico ha consentito di migliorare la fruizione delle notizie, velocizzandone la propagazione tramite i cosiddetti fenomeni virali; inoltre, la presenza di motori di ricerca e, in generale, l'ampio spazio sul Web, hanno consentito la creazione di nuove testate giornalistiche e fonti di informazione, alcune di natura puramente digitale.
L'altra faccia della medaglia è che una tale facilitazione dei processi sopra menzionati ha contribuito alla crescita del fenomeno della \textit{disinformazione}.
Tale fenomeno prende forma in uno scenario caratterizzato da una crescente quantità di dati, in cui gli utenti che utilizzano Internet si ritrovano esposti a un numero di informazioni sempre più grande.
In un contesto simile è facile intuire come il livello di distrazione aumenti sensibilmente; inoltre, verificare le fonti delle notizie è un processo che richiede sempre più attenzione e, ovviamente, uno sforzo attivo da parte di chi le legge.

L'aspetto problematico della disinformazione è che costituisce un fenomeno, spesso, intenzionale e guidato da interessi che coinvolgono principalmente il mondo della politica, dal momento che la veicolazione delle \textit{fake news} può mirare all'alterazione dell'opinione pubblica. 
Con il passare degli anni, le tipologie di fake news sono aumentate e la loro qualità, a volte, risulta paragonabile a quella delle notizie affidabili. Il problema considerato, dunque, è complesso, in quanto gli elementi in questione sono costruiti per essere ingannevoli per gli utenti.
Inoltre, nel rapporto 2020 dei servizi italiani di Intelligence viene segnalato come durante la pandemia di Covid-19 ci sia stato un forte aumento di fake news relativamente all'ambito sanitario.
\\
\\
Alla luce delle considerazioni appena fatte, si propone un sistema di rilevazione delle fake news con l'obiettivo finale di ricavare dei punteggi di \textit{fakeness} per le stesse in modo da fornire agli utenti uno strumento di supporto alla valutazione delle notizie durante la loro lettura.
La motivazione di questa scelta risiede nel fatto di optare per una soluzione poco invasiva: un punteggio simile costituirebbe un modo per educare le persone ad essere critiche piuttosto che per censurare articoli. Il vantaggio di questa scelta risulta evidente nel momento in cui un sistema di questo tipo possa commettere degli errori.
Il sistema si basa sulla logica fuzzy e sull'idea che le notizie siano riconducibili a un insieme fuzzy delle fake news, di cui si può calcolare il grado di appartenenza associato. Tale grado rappresenterebbe il valore di \textit{fakeness} ricercato per le notizie.
La soluzione proposta è stata, quindi, sviluppata nel lavoro di questa tesi magistrale nel Dipartimento di Informatica dell'Università degli Studi di Milano e si basa su un algoritmo di apprendimento supervisionato per l'induzione di insiemi fuzzy a partire da una tecnica di support vector clustering.
\\
\\
La tesi realizzata è strutturata come segue:
all'interno del Capitolo 1 viene introdotto il problema considerato e le tecniche allo stato dell'arte; successivamente, nel Capitolo 2 viene presentata la soluzione proposta ad alto livello mentre nel Capitolo 3 viene descritta nel concreto la sua implementazione. All'interno del Capitolo 4, invece, vengono illustrati gli esperimenti effettuati con i relativi risultati. Infine, sulla base di tali risultati, sono state tratte le conclusioni finali ed è stata stilata una lista di possibili sviluppi futuri.

\chapter{Stato dell'arte}
\label{Capitolo 1}
\onehalfspacing

Il capitolo che apre questa tesi è inerente allo stato dell'arte e si compone come segue: il Paragrafo~\ref{fakenews} descrive il problema delle fake news, introducendo la disciplina del fact-checking; il Paragrafo~\ref{nlp} riporta le tecniche di elaborazione del linguaggio naturale, soffermandosi sulla gestione del rumore, sulla rielaborazione del testo e sull'embedding per convertire tale testo in un dato di tipo numerico per la fase di preprocessing. 
Il Paragrafo~\ref{insiemifuzzy} presenta il concetto degli insiemi fuzzy e fornisce una motivazione del perché essi siano uno strumento adeguato per il problema delle fake news.
Infine, il Paragrafo~\ref{induzione} porta alla luce le principali componenti del processo di induzione considerato.

\section{Fake news} \label{fakenews}
L'avvento dei news media e dei social media ha portato a una proliferazione e a un consumo crescenti di notizie.
In generale, la circolazione di notizie su questi canali ha fatto sì che aumentassero esponenzialmente le cosiddette \textit{fake news}.

Per fake news si intendono quelle notizie riportanti fatti che volutamente non corrispondono alla realtà.
In questo senso, si evidenzia la necessità di riconoscere e combattere questo fenomeno con l'obiettivo di contrastare il fomentare dell'odio, un'arma che al giorno d'oggi può essere usata come carburante di diffamazione, lucro, terrorismo e xenofobia \cite{4}.

Il problema di questo tipo di notizie è che possono mischiarsi con tutte le altre, portando il lettore a confondere un fatto realmente accaduto con uno volutamente alterato per un secondo fine.
Inoltre, esiste un'intrinseca difficoltà nel valutare la veridicità di una notizia sia per il problema di attingere a fonti attendibili, sia per la natura stessa del testo scritto.

\subsection{Fact-checking} \label{factchecking}
Al di là di quale sia il secondo fine di chi diffonde fake news, il nemico numero uno del giornalismo è la disinformazione.
Per questo motivo, un lavoro da sempre svolto da giornalisti, e non, è quello del \textit{fact-checking}: si tratta di una serie di attività mirate alla verifica accurata e puntuale delle fonti.
Secondo questo criterio, la verifica delle sorgenti di informazione avrebbe il vantaggio di validare i fatti, non lasciando spazio ad avvenimenti non confermati da fonti autorevoli.
Questo approccio, però, presenta delle criticità, ad esempio l'assunzione che le fake news non abbiano fonti: 
esistono notizie che, pur basandosi su fatti realmente accaduti, possono esasperare aspetti apparentemente secondari che, invece, possono alterare la narrazione principale, fino a sconvolgerla.

Un altro aspetto è che non è così semplice stabilire con certezza quando una fonte sia autorevole e quando no; inoltre, è opinabile assumere a priori che la stessa fonte considerata autorevole non commetta mai a sua volta errori di questo tipo.

Uno dei problemi maggiori, oltre al fatto che la verifica della veridicità di una notizia richieda del tempo, è che la smentita non finisce mai per avere la stessa risonanza e visibilità della notizia falsa. Questo amplifica l'esigenza di trovare un metodo per prevenire il problema, piuttosto che risolverlo a posteriori.

La necessità di arginare questo fenomeno ha spinto i media, soprattutto tradizionali, a impegnarsi in un costante lavoro di fact-checking, spesso lungo, impegnativo e reso ancora più difficile dal fatto che le varie realtà tendono ad agire in maniera autonoma.

Ci si chiede, quindi, se il progresso in ambito informatico possa contribuire ad arginare il problema in maniera efficace.

In letteratura sono stati fatti vari studi per la cosiddetta automazione del fact-checking \cite{5, 6, 8, 9, 10, 11, 12, 13, 17}; inoltre, in \cite{15, 16, 21} sono stati proposti approcci che, principalmente, si suddividono nelle categorie focalizzate rispettivamente sui metodi \textit{content-based} e \textit{context-based}.
Mentre il primo approccio lavora sul contenuto testuale a livello sintattico, il secondo tenta di estrapolarne il contesto, lavorando a livello semantico.

\section{Elaborazione del linguaggio naturale} \label{nlp}
L'automazione dell'analisi lessicale fa parte dell'ampia branca dell'informatica che prende il nome di \textit{Natural Language Processing} (NLP) che, tra le sue varie declinazioni, presenta degli interessanti strumenti per poter estrarre delle feature a partire da dati testuali come le notizie.


\subsection{Tecniche di gestione del rumore} \label{clean}
Come nella stragrande maggioranza dei dataset, anche in quelli testuali è presente del rumore, sia a ``basso livello'' nel contenuto dell'informazione, sia ad ``alto livello'' nella forma dei dati che si sta utilizzando.

Possono essere molte le motivazioni che portano un dataset a presentare errori, anomalie o rumore al suo interno: 
nel caso di osservazioni raccolte manualmente può verificarsi una componente di errore umano; dualmente, dataset generati automaticamente (ad esempio tramite dati raccolti da sensori) possono presentare delle anomalie e produrre risultati imprevisti.

Nel caso dei dati trattati in questa tesi, inoltre, si ha a che fare con informazioni provenienti dal Web, tipicamente ricavate tramite Web Scraping\footnote{Si tratta di una tecnica eseguita automaticamente da un programma software che estrae e memorizza i dati dalle pagine Web.}. Per questa ragione esiste una maggior probabilità di incontrare osservazioni di natura digitale ricavate da pagine HTML o, addirittura, influenzate dal tipo di codifica scelto per rappresentare caratteri speciali.

In letteratura sono presenti numerose tecniche per gestire il rumore; seguendo la distinzione fatta all'inizio di questo paragrafo relativamente alle diverse tipologie di rumore, è possibile fare un elenco.
\\
\\
Ad \textit{alto livello} si gestisce la presenza di:
\begin{itemize}
    \item valori mancanti,
    \item osservazioni duplicate,
    \item osservazioni vuote,
\end{itemize}

mentre a \textit{basso livello}, tipicamente, si riscontrano:
\begin{itemize}
    \item caratteri speciali,
    \item URL,
    \item parole contenenti numeri,
    \item punteggiatura,
\end{itemize}

e altri numerosi casi che potrebbero essere inseriti nella lista.
\subsection{Tecniche per rielaborare il testo}
Esistono delle procedure per riadattare il testo in una forma più conveniente per la sua successiva elaborazione da parte di algoritmi di Machine Learning.

Vari studi nel campo dell'Information Retrieval \cite{2, 22}, infatti, hanno dimostrato che tecniche come lo \textit{stemming}, la \textit{lemmatizzazione} e la rimozione delle cosiddette \textit{stop word} possono migliorare sensibilmente i risultati ottenuti da tali modelli.

\paragraph{Stemming} Lo stemming consiste nell'individuare e rimuovere il prefisso e il suffisso delle parole, in modo da ricavarne la radice, ne viene mostrato un esempio in Tabella~\ref{stemming}.
\begin{table}
\centering
 \begin{tabular}{|c|c|c|} 
 \hline 
 \textbf{Forma} & \textbf{Suffisso} & \textbf{Radice}
\\ [0.5ex] 
\hline
pront\textbf{o} & \textbf{-o} & \textbf{pront} \\
pronunc\textbf{erà} & \textbf{-erà} & \textbf{pronunc} \\
pronunc\textbf{ia} & \textbf{-ia} & \textbf{pronunc} \\
 \hline
\end{tabular}
\caption{Esempio di stemming.}
\label{stemming}
\end{table}

\paragraph{Lemmatizzazione} La lemmatizzazione prende in considerazione l'analisi morfologica delle parole ricorrendo a dettagliati dizionari che l'algoritmo utilizza per ottenere il lemma associato.
Un esempio di questa tecnica viene mostrato in Tabella~\ref{lemmatization}.
\begin{table}
\centering
 \begin{tabular}{|c|c|c|} 
 \hline 
 \textbf{Forma} & \textbf{Informazione morfologica} & \textbf{Lemma}
\\ [0.5ex] 
\hline
ragazze & femminile plurale di \textbf{ragazzo} & \textbf{ragazzo} \\
studia & terza persona singolare, presente del verbo \textbf{studiare} & \textbf{studiare} \\
studiando & gerundio del verbo \textbf{studiare} & \textbf{studiare} \\
 \hline
\end{tabular}
\caption{Esempio di lemmatizzazione.}
\label{lemmatization}
\end{table}

\paragraph{Rimozione delle stop word}
Gli articoli, le proposizioni, le congiunzioni e gli aggettivi sono esempi tipici di stop word. Queste parole hanno solitamente un'alta frequenza nei documenti ma non aggiungono alcun valore semantico al testo in quanto sono tipicamente necessarie per la grammatica del linguaggio; pertanto, rimuoverle è una soluzione che viene spesso adottata per ridurre il carico computazionale dell'algoritmo che elabora il testo.
\\
\\
Naturalmente, le considerazioni fatte per queste tre tecniche valgono per qualsiasi idioma; tipicamente, le librerie che le implementano presentano delle interfacce per specificare con quale lingua si intende lavorare in maniera da ricorrere a opportuni dizionari.

\subsection{Tecniche di embedding} \label{embedding}
Gli algoritmi di Machine Learning vengono eseguiti per generare dei modelli che sono in grado di fare delle predizioni. Tuttavia, sia i modelli che gli algoritmi in questione necessitano di un input numerico; dal momento che l'obiettivo di questa tesi è trattare le notizie, cioè un dato tipo di testuale, è fondamentale ricorrere a delle tecniche di embedding che intervengono proprio per trasformare il dato testuale in dato numerico.
Concretamente, questo equivale ad estrarre delle feature che caratterizzano le notizie.

Due famose tecniche di embedding sono \textit{Word2Vec} e \textit{Doc2Vec}, il cui meccanismo viene illustrato nei paragrafi che seguono.
\subsubsection{Word2Vec} \label{w2v}
Word2Vec è un algoritmo che ha l'obiettivo di trasformare le parole in vettori numerici all'interno di uno spazio  di dimensione prefissata \cite{3}.
Un corpus è composto da documenti e ogni documento è composto da parole; ciascuna di queste parole, tramite Word2Vec, viene trasformata in un vettore di lunghezza $h$, dove $h$ indica il numero di feature numeriche che vengono considerate. 

Word2Vec si basa sull'utilizzo di reti neurali, dei modelli computazionali composti da neuroni artificiali che emulano in maniera semplificata il comportamento dei neuroni biologici \cite{38}, ed è principalmente implementato tramite due modelli: \textit{Skip-Gram} e \textit{CBOW} (Continuous Bag of Words), che vengono descritti qui di seguito.

Per entrambi i modelli l'input è un corpus di documenti, le cui parole vengono distinte in \textit{token}; ciascun token viene codificato con una rappresentazione one-hot\footnote{La codifica one-hot è un processo che viene applicato ai dati categorici per convertirli in stringhe binarie da utilizzare negli algoritmi di apprendimento automatico. In questo caso i dati categorici sono le parole e ciascuna di esse può essere rappresentata come il vettore binario di tutti i termini presenti nel documento. Se $n$ è il numero di parole, allora il vettore sarà composto da $n-1$ zeri e da un uno, a indicare quale token venga effettivamente rappresentato.}.

Per differenziare le due soluzioni, è necessario introdurre il concetto di contesto, poiché Skip-Gram e CBOW lavorano in due direzioni speculari:
mentre il primo modello si pone l'obiettivo di predire le parole di contesto a partire dal token corrente, il secondo ha lo scopo di predire il token corrente da una finestra di parole di contesto.

Quando una parola $P$ appare in un testo, il suo contesto è quel set di parole che gli appaiono accanto, data una finestra di analisi precedentemente impostata. I molteplici contesti in cui la parola $P$ viene utilizzata servono a costruire una rappresentazione dell’uso di $P$.

Ogni parola viene associata a un vettore denso, ossia una scala di valori numerici vettoriali che, a sua volta, viene messa in associazione con vettori di parole che appaiono in contesti simili, costruendo quelli che vengono definiti \textit{word vectors}.

\paragraph{Skip-Gram}
Si stabilisce una finestra di dimensione $m$ e si scorre ogni token andando a vedere i termini in prossimità, osservando quelli all'interno del raggio $m$.
\begin{figure}
    \centering
    \includegraphics[scale = 0.7]{images/skip-gram.png}
    \caption{Esempio di Skip Gram: in verde il token corrente, in rosso la finestra di contesto grande 5 token.}
    \label{skipgram}
\end{figure}
Per esempio, come illustrato in Figura~\ref{skipgram}, se \textit{folla} è il token corrente, con $m = 5$ il confronto avviene con \{\textit{giornata}, \textit{di}, \textit{ieri}, \textit{una}, \textit{grossa}, \textit{si}, \textit{è}, \textit{riunita}, \textit{in}, \textit{piazza}\}.

L'idea è quella di cercare di costruire il mapping tra $X$, i token correnti (ad esempio \textit{folla}), e $y$, i token estratti dalla finestra di contesto (per esempio \textit{piazza}).

Seguendo la notazione tipica di un problema di apprendimento supervisionato, i token presi dalla finestra di contesto sono la variabile target da predire, apprendendo il tipo di relazione che sussiste tra  $X$ e $y$.
Per farlo, come accennato in precedenza, si utilizza una rete neurale, passando la rappresentazione one-hot del dato a un'unità softmax\footnote{Softmax è una possibile funzione di attivazione dello strato di output della rete neurale. In realtà, vale la pena menzionare altre due varianti di criteri di addestramento applicabili, come Negative sampling e Hierarchical Softmax, con diverse implicazioni riguardo efficienza e onere computazionale.}, una funzione che permette di calcolare la distribuzione di probabilità dei valori possibili nella classificazione multi-classe.

\paragraph{CBOW}
Dualmente a Skip-Gram, CBOW si occupa di predire il token corrente a partire dalla finestra di contesto.

Come mostrato in Figura~\ref{cbow}, si predice con quale probabilità si ottenga il token \textit{folla} a partire dalla finestra \{\textit{giornata}, \textit{di}, \textit{ieri}, \textit{una}, \textit{grossa}, \textit{si}, \textit{è}, \textit{riunita}, \textit{in}, \textit{piazza}\}.
\begin{figure}
    \centering
    \includegraphics[scale = 0.7]{images/cbow.png}
    \caption{Esempio di CBOW: in rosso il token corrente, in verde la finestra di contesto grande 5 token.}
    \label{cbow}
\end{figure}
Anche in questo caso viene addestrata una rete neurale utilizzando il metodo del gradiente discendente per ottenere l'embedding.
\\
\\
In generale, data la natura di queste tecniche, per ottenere una rappresentazione compatta dell'intero documento si rende necessario l'utilizzo di una tecnica di aggregazione.
In questa fase ogni documento $i$ è formato da $n_i$ parole ed ogni parola è rappresentata da un vettore di $h$ feature. L'aggregazione interviene per comprimere ciascuno di questi vettori in un valore che sia rappresentativo della corrispondente parola, ottenendo, così, un vettore per l'intero documento.

Così come in tante altre applicazioni dell'informatica, i metodi di aggregazione sono molteplici, ad esempio la media aritmetica, la mediana, l'aggregazione del kernel di Fisher \cite{19}.

Fare la media tra vettori significa poter agire su due dimensioni: una di queste prevede di sommare prima gli elementi di ciascun vettore e poi di dividere per il numero di elementi, ottenendo $n_i$ valori medi. In realtà, questa soluzione è scomoda per la successiva elaborazione dei dati, dal momento che ogni documento $i$ può avere un numero variabile di parole e, dunque, si otterrebbero vettori di lunghezza diversa per il corpus finale.

La soluzione utilizzata, invece, consiste nel sommare i primi elementi di tutti i vettori, i secondi, i terzi e così via, per poi dividere per il numero di parole. Così facendo si preserva la rappresentazione tramite feature, perché si ottiene per ogni documento un vettore lungo $h$; un discorso analogo vale per la mediana.

L'aggregazione del kernel di Fisher, invece, propone una rappresentazione che si basa su quanto il vettore osservato si discosti dal modello generativo GMM (Gaussian Mixture Model) \cite{37}. Tale discostamento è una misura di distanza che viene calcolata tramite il gradiente di una funzione di verosimiglianza; i vettori ottenuti prendono il nome di \textit{Fisher vectors}.

\subsubsection{Doc2Vec} \label{d2v}
Doc2Vec nasce come evoluzione di Word2Vec: in questo caso, anziché lavorare a livello di ogni singola parola, si determina direttamente una rappresentazione vettoriale per l'intero documento \cite{24}.
Questo comporta chiaramente il raggiungimento del risultato senza ricorrere a un metodo di aggregazione dei valori.

Conosciuto anche come \textit{Paragraph Vector}, Doc2Vec si articola in due principali implementazioni: \textit{PV-DM} (Distributed Memory) e \textit{DBOW} (Distributed Bag of Words).

\paragraph{PV-DM} 
Come per Word2Vec, il task che viene fatto ripetutamente è quello di predire la parola successiva nella frase. I vettori delle parole e i vettori dei capoversi sono chiamati a contribuire a tale predizione.

Ogni capoverso viene mappato in un vettore, rappresentato da una colonna nella matrice $D$, così come il vettore di ogni parola è rappresentato da una colonna della matrice $P$ (Figura~\ref{pvdm}). I vettori menzionati vengono mediati o concatenati e, a loro volta, essi vengono passati alla rete neurale per prevedere la parola centrale.

La rappresentazione del capoverso è ciò che effettivamente distingue questa tecnica da Word2Vec in quanto, pur agendo come un'altra parola, svolge il ruolo di memoria per ricordare cosa manca al contesto corrente; da qui, il nome \textit{Distributed Memory}.
\begin{figure}
    \centering
    \includegraphics[scale = 0.3]{images/pvdm.png}
    \caption{Architettura in PV-DM (D è la matrice dei documenti e P la matrice delle parole).}
    \label{pvdm}
\end{figure}
La matrice del capoverso ha, infatti, gli embedding per i capoversi ``visti'', allo stesso modo in cui i modelli Word2Vec apprendono gli embedding per le parole. Per i capoversi non visualizzati, invece, il modello viene nuovamente eseguito più volte attraverso la discesa del gradiente per generare un vettore del documento. 

\paragraph{DBOW}
L'architettura DBOW non utilizza le parole di contesto ma effettua la predizione direttamente dalle parole campionate dal capoverso.
\begin{figure}
    \centering
    \includegraphics[scale = 0.45]{images/dbow.png}
    \caption{Architettura in DBOW.}
    \label{dbow}
\end{figure}
Ne risulta un'architettura (Figura~\ref{dbow}) simile a PV-DM ma con il primo livello costituito unicamente dalla matrice dei capoversi.

\section{Insiemi fuzzy} \label{insiemifuzzy}
Dal momento che questa tesi si pone l'obiettivo di analizzare le notizie e, nello specifico, di valutare un criterio per individuare quelle fake, è importante introdurre il concetto degli \textit{insiemi fuzzy}.

Diversamente da quanto accade per gli insiemi classici in cui l'appartenenza  è un concetto binario che viene espresso da un valore di verità, negli insiemi fuzzy esso viene quantificato da un valore continuo tra 0 e 1.
Tale valore prende il nome di \textit{grado di appartenenza}.

In altre parole, gli insiemi fuzzy introducono un significato associato all'appartenenza a un insieme che è più granulare rispetto alla tradizionale dicotomia binaria degli insiemi classici, ottenendo valori che indicano anche se una certa espressione sia molto vera, poco vera o mediamente vera.

Nel campo della Sentiment Analysis \cite{18, 25}, per esempio, cercare di determinare le emozioni contenute in un testo rientra in questo tipo di problemi; sarebbe limitante, infatti, considerare unicamente se un tweet o un post esprima felicità o meno, oppure se sia vero o falso che ci sia rabbia nelle parole del messaggio di una persona.

Più realisticamente, esistono componenti più o meno forti di ciascuna di queste emozioni che si mischiano e che formano, complessivamente, un testo.
Tali misture, inoltre, causano spesso dell'incertezza che, di fatto, è la ragione per cui il problema risulta più complesso e, allo stesso tempo, affascinante.

Vari studi, inoltre, hanno evidenziato come l'ambiguità sia una caratteristica intrinseca del linguaggio umano \cite{26, 27}.
Rimanendo nell'esempio della Sentiment Analysis, la presenza di testi con emozioni ambigue è oggi oggetto di ricerca.

Alla luce di tutto ciò, questa tesi propone di modellare la rilevazione delle fake news come un problema fuzzy, assumendo che le notizie siano associate a insiemi di questo tipo.

L'obiettivo è quello di ricavare un modello di apprendimento supervisionato in grado di produrre il grado di appartenenza a tale insieme a partire dal suo contenuto, con l'ambizione finale di ottenere un punteggio di \textit{fakeness} per ogni notizia;
potenzialmente, tale punteggio rappresenta quanto la notizia sia fake. Da questo punto di vista è stato fatto un lavoro simile \cite{35} che si basa su un approccio fuzzy ma che ricava l'affidabilità del contenuto a partire dalla fonte dell'informazione.

\section{Induzione di funzioni di appartenenza} \label{induzione}
In letteratura è stato proposto un algoritmo che fa uso di una procedura originariamente nata come tecnica di support vector clustering per poter indurre la funzione di appartenenza dei punti a un certo insieme fuzzy \cite{1}.

I punti fondamentali di questo approccio riguardano determinare la forma di tale insieme fuzzy e inferire i parametri della sua funzione di appartenenza.

\subsection{Funzione di appartenenza} \label{membership}
Il concetto di funzione di appartenenza si colloca nella teoria degli insiemi e corrisponde alla funzione caratteristica di un insieme.

Fissando l'insieme $A$ e lo spazio $X$, la sua funzione di appartenenza $\mu_A$ tale che $Dom(\mu_A) = X$ è definita come:
\begin{equation}
    \mu_A(x)= \begin{cases} 1 & \mbox{se } x \in A, \\ 0 & \mbox{altrimenti.} \end{cases}
\end{equation}
Secondo la teoria classica degli insiemi, infatti, un insieme è definito come qualunque aggregato (o collezione) di oggetti per il quale sia sempre possibile decidere se un generico oggetto appartiene oppure no all'aggregato stesso\footnote{In realtà, questa definizione è stata dimostrata come fallace verso la fine del XIX secolo con quella che venne definita la \textit{crisi dei fondamenti della matematica}. Tale crisi produsse una serie di paradossi, tra cui il famoso \textit{paradosso di Russell} da cui venne derivato il \textit{paradosso del barbiere}. Per rigorosità, sarebbe più opportuno usare una definizione assiomatica degli insiemi, tuttavia, al fine di non rendere prolisso il richiamo alla notazione insiemistica classica, è stata preferita una definizione informale.}.

Quando la funzione di appartenenza è booleana, perché si basa su due soli possibili valori (0 o 1), si parla di insieme \textit{crisp}. Nell'ambito delle notizie, questo corrisponderebbe a classificare ogni notizia come completamente fake o no, a seconda del fatto che appartenga o meno all'insieme delle notizie false.

Lo scopo di questo lavoro, invece, è di rappresentare lo stesso concetto ma in maniera sfumata, producendo informazioni su \textit{quanto} una notizia sia falsa.
Tale rappresentazione ha il vantaggio di poter indicare se una notizia sia più o meno fake di un'altra.
Per questa ragione si introduce il concetto di \textit{grado di appartenenza} con l'idea di base che il confine tra oggetti appartenenti e non appartenenti all'insieme non sia così ben definito.

Ci si concentra, quindi, su una funzione di appartenenza $\mu_A: X \rightarrow [0,1]$ che associa a ogni elemento dell'universo considerato un numero reale compreso tra 0 e 1, dove $X$ è il dominio di $\mu_A$.

Formalmente, si può asserire che:
\begin{itemize}
    \item se $\mu_A(x) = 1$ allora $x$ appartiene all'insieme $A$,
    \item se $\mu_A(x) = 0$ allora $x$ non appartiene all'insieme $A$,
    \item se $0 < \mu_A(x) < 1$ allora $x$ appartiene parzialmente ad $A$ con grado espresso da $\mu_A(x)$.
\end{itemize}
Esempi di concetti fuzzy sono \textit{giovane}, \textit{ricco}, \textit{alto}, mentre non lo sono \textit{fratello}, \textit{studente}, \textit{professore}.

Per determinare il valore di $\mu_A$ vengono definiti diversi tipi di funzioni di appartenenza: funzione sigma, funzione triangolare, trapezoidale, S-Shape, e altre ancora (Figura~\ref{membership_functions}). 
\begin{figure}
    \centering
    \includegraphics[scale = 0.7]{images/membership_functions.png}
    \caption{Alcuni tipi diffusi di funzioni di appartenenza - da \cite{30}.}
    \label{membership_functions}
\end{figure}
\subsection{Metodi kernel} \label{kernel}
I metodi kernel sono degli oggetti matematici ampiamente utilizzati in congiunzione con le support vector machine \cite{28} per problemi di natura non lineare, tuttavia è applicabile anche a numerosi altri contesti, ad esempio la \textit{kernel Principal Component Analysis} \cite{29}.

Questa metodologia prende il nome di \textit{kernel trick} e consiste nel mappare i punti dallo spazio originale $\mathcal X$ a uno spazio $\mathcal H$ a dimensionalità superiore o infinita, dove il problema diventa lineare e i dati risultano linearmente separabili.
Tale tecnica si appoggia a una funzione kernel, una funzione simmetrica 
\begin{equation}
    k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}
\end{equation} 
tale che in $\mathcal{H}$, detto \textit{spazio delle feature}, è definito un prodotto scalare $<$ ·, · $>$ ed esiste una funzione $\mathit{\Phi }: \mathcal{X} \rightarrow \mathcal{H}$ per cui:
\begin{equation}
    k(x,y) = <\mathit{\Phi}(x), \mathit{\Phi}(y)>
\end{equation}
dove $x,y \in \mathcal{X}$.
I dati vengono, quindi, rappresentati tramite dei confronti di coppie, misurando l'equivalente di una misura di similarità: all'aumentare di $k(x,y)$, $x$ e $y$ sono da considerarsi maggiormente  ``simili''.
Inoltre, tramite l'utilizzo di questo metodo, la dimensionalità dei dati tende a diventare poco influente, ragione per cui, spesso, metodi come questi non soffrono della \textit{maledizione della dimensionalità} \cite{39}.
Esistono diversi tipi di kernel, ad esempio lineare, polinomiale, iperbolico, gaussiano e molti altri ancora.

\subsection{Tipi di kernel}
In letteratura è presente un'importante varietà di kernel ed essi si distinguono per il tipo di funzione $k(x,y)$ utilizzata. Seguono, dunque, alcuni esempi dei kernel più diffusi:

\paragraph{Kernel lineare}
Il kernel lineare usa la funzione
\begin{equation}
    k(x,y) = x \cdot y = \sum\limits_{i=1}^n x_iy_i
\end{equation}
dove $n$ è la dimensione comune di $x$ e $y$.
La proiezione dell'iperpiano dallo spazio $\mathcal{H}$ allo spazio $\mathcal{X}$ corrisponde a un iperpiano.

\paragraph{Kernel polinomiale}
Il kernel polinomiale usa la funzione
\begin{equation}
    k(x,y) = (x \cdot y + 1)^d
\end{equation}
dove $d$ è il grado polinomiale del kernel.
In questo caso l'iperpiano equivale a una superficie polinomiale di grado al più $d$.

\paragraph{Kernel polinomiale omogeneo}
Il kernel polinomiale omogeneo usa la funzione
\begin{equation}
    k(x,y) = (x \cdot y)^d
\end{equation}
dove $d$ è il grado polinomiale del kernel. La proiezione dell'iperpiano, qui, corrisponde a una superficie polinomiale di grado esattamente uguale a $d$.

\paragraph{Kernel gaussiano}
Il kernel gaussiano usa la funzione
\begin{equation}
    k(x,y) = e^{-\frac{||x-y||^2}{2\sigma^2}}
\end{equation}
dove $\sigma$ è la deviazione standard. Questo tipo di kernel è una combinazione lineare di infiniti kernel polinomiali di grado crescente. Lo spazio delle feature indotto dal kernel gaussiano, infatti, è dimensionalmente infinito. L'iperpiano in tale spazio equivale nello spazio originale a una sovrapposizione di curve gaussiane, la cui deviazione standard è regolata da $\sigma$.

\paragraph{Kernel basato su tangente iperbolico}
Il kernel basato su tangente iperbolico usa la funzione
\begin{equation}
    k(x,y) = \tanh(\alpha x \cdot y + \beta)
\end{equation}
dove $\alpha$ e $\beta$ sono rispettivamente i parametri di scala e offset. La funzione in questione ha origine nell'ambito delle reti neurali; si osserva, inoltre, che un modello basato su support vector machine che usa questa funzione di attivazione sia equivalente a una rete neurale feedforward a due strati.

\chapter{Soluzione proposta}
\label{Capitolo 2}
\onehalfspacing
Questo capitolo presenta la soluzione che viene proposta in questa tesi.
Tale soluzione consiste in un sistema di apprendimento supervisionato a partire dall'algoritmo $\bm{\mu}$\textbf{-learn} \cite{1}, il cui funzionamento viene descritto nel Paragrafo~\ref{mulearn}.

Segue nel Paragrafo~\ref{evaluation} l'illustrazione dei criteri di valutazione dei modelli e nel Paragrafo~\ref{generalization} la maniera con cui viene misurata la loro bontà di generalizzazione.

Nel Paragrafo~\ref{sistema}, invece, si mostra l'utilizzo che è stato fatto dell'algoritmo sopracitato all'interno del sistema proposto in questo capitolo, delineando le fasi che si susseguono nel corso degli esperimenti. Infine, nel Paragrafo~\ref{predictors} vengono mostrati i due tipi di predittore che sono stati proposti per tali esperimenti.

\section[\texorpdfstring{L'algoritmo $\mu$-learn}%
                        {mu-learn}]% % choose text-only material here
        {L'algoritmo $\bm{\mu}$-learn}  % note use of \bm ("bold math")
\label{mulearn}
$\mu$-\textit{learn} è un algoritmo di Machine Learning supervisionato che utilizza una procedura originariamente nata nel contesto del support vector clustering~\cite{23}.
Esso si colloca nel gruppo di tecniche che lavora su insiemi fuzzy, trattando un problema di ottimizzazione non lineare.

La diversità delle funzioni di appartenenza presenti in letteratura permette una vasta sperimentazione e i parametri associati a tali funzioni possono essere regolati in modo da ottenere forme differenti per gli insiemi fuzzy formati.
Il nome di questo algoritmo suggerisce il suo obiettivo: apprendere la funzione di appartenenza $\mu$ a un determinato insieme fuzzy e generare l'associato modello predittivo.

\subsection{Support vector clustering modificato}\label{svcmodified}
L'assunzione iniziale è quella di disporre di un dataset $S$ di $m$ osservazioni tale che
\begin{equation}
    S = \{(\mathbf{x_i}, y_i), \; \forall\,i=1, ..., m\}
\end{equation}
in cui $\mathbf{x_i} \in \mathbb{R}^h$ è l'$i$-esimo vettore delle feature e $y_i$ è l'etichetta target che si vuole predire; tale etichetta può essere rappresentata da dei valori binari in $\{0,1\}$ o dai gradi di appartenenza $\mu_i$ a un insieme fuzzy sconosciuto $A$.
L'obiettivo è proprio individuare tale insieme e determinare la funzione di appartenenza $\mu_A$ per approssimarne il grado.

L'algoritmo descritto nel Paragrafo~\ref{mulearn} parte dalla tecnica di support vector clustering presente in letteratura~\cite{23} e la estende per l'individuazione di insiemi fuzzy. Si tratta di un algoritmo di clustering non parametrico che affronta il problema dell'identificazione del minimo raggio $R$ dell'ipersfera di centro $a$ che include le immagini della maggior parte di un insieme di punti; in particolare, i punti che si trovano sulla superficie di tale ipersfera sono chiamati \textit{support vector} e, nello spazio delle feature, essi dividono i dati in cluster.
Tali punti vengono esclusi o inclusi da $\mu$-learn in funzione dei loro gradi di appartenenza a un insieme fuzzy, sulla base della distanza tra le immagini dei punti e $a$.

Il problema di ottimizzazione non lineare a cui si accennava nel Paragrafo~\ref{mulearn} riguarda, quindi, la minimizzazione del raggio di questa ipersfera.

Quelli che seguono sono i tre vincoli del problema e si distinguono dalla tecnica originale proprio per l'introduzione di $\mu_i$:
\begin{equation}\label{vincolo}
    \mu_i || \mathit{\Phi}(x_i) - a ||^2 \leq \mu_iR^2 + \xi_i \;,
\end{equation}
\begin{equation}\label{vincolo_2}
    (1 - \mu_i) || \mathit{\Phi}(x_i) - a ||^2 \geq (1 - \mu_i)R^2 - \tau_i \;,
\end{equation}
\begin{equation}\label{vincolo_3}
    \xi_i \geq 0, \tau_i \geq 0 \;.
\end{equation}
In~\ref{vincolo} il primo vincolo richiede che le immagini dei punti attraverso $\mathit{\Phi}$ abbiano una distanza dal centro minore o uguale di una quantità che è tanto più piccola quanto più è elevato il relativo $\mu_i$. Il secondo vincolo in~\ref{vincolo_2} richiede, invece, che al diminuire di $\mu_i$, le immagini si trovino a una distanza che tende ad aumentare. Infine, il vincolo~\ref{vincolo_3} impone dei valori non negativi per le variabili slack che rilassano il problema.

Il problema di ottimizzazione originale viene, quindi, esteso minimizzando $R^2 + C\sum(\xi_i + \tau_i)$ tramite (\ref{vincolo}-\ref{vincolo_3}), dove $C$ è un iperparametro, e ricavando la soluzione tramite un solver.
La fase successiva è quella di indurre il valore della funzione di appartenenza a tale insieme.

\subsection{Fuzzificatore} \label{fuzzificatore}
In letteratura i termini \textit{fuzzificazione} e \textit{defuzzificazione} indicano rispettivamente il passaggio da una quantità crisp a una quantità fuzzy e viceversa.
Tale passaggio può avvenire in differenti modi che dipendono dal tipo di fuzzificatore che si utilizza. 
In altre parole, si stabilisce il modo in cui vengono fatti decrescere i valori della funzione di appartenenza dal valore 1 fino a 0. Geometricamente, questo equivale a definire una certa misura di distanza nello spazio in cui i punti vengono racchiusi; con grado di appartenenza maggiore essi saranno raggruppati più densamente nell'ipersfera di cui l'algoritmo minimizza il raggio, dualmente con grado minore, i suddetti punti saranno più distanti dall'insieme.
\begin{figure}
    \centering
    \includegraphics[scale=0.52]{images/fuzzifiers.png}
    \caption{Tipi di fuzzificatore implementati.}
    \label{fuzzifierstype}
\end{figure}
Nel caso di $\mu$-learn, sono stati implementati diversi tipi di fuzzificatore (Figura~\ref{fuzzifierstype}), ciascuno caratterizzato dall'apprendimento di un diverso profilo della funzione di appartenenza. Per profilo si intende la funzione associata 
\begin{equation}
    p: \mathbb{R}^+ \rightarrow [0,1]
\end{equation}
che ritorna i gradi di appartenenza a partire dai suoi argomenti nello spazio delle feature. Ogni tipo di fuzzificatore determina, quindi, una specifica distribuzione per $p$.

\paragraph{Fuzzificatore crisp}
Il fuzzificatore crisp usa una funzione basata su soglia $r_{crisp}$ che viene tipicamente posta uguale al quadrato del raggio della sfera appresa o indotta tramite interpolazione.
\begin{equation}
    p(r)= \begin{cases} 1 & \mbox{se } r \leq r_{crisp}, \\ 0 & \mbox{altrimenti.} \end{cases}
\end{equation}

\paragraph{Fuzzificatore lineare}
In questo tipo di fuzzificatore, il grado di appartenenza decresce linearmente, per cui
\begin{equation}
    p(r)= \begin{cases} 1 & \mbox{se } r \leq r_1, \\l(r) & \mbox{se } r_1 \leq r \leq r_0, \\ 0 & \mbox{altrimenti.} \end{cases}
\end{equation}
in cui $l$ è una funzione lineare.

\paragraph{Fuzzificatore esponenziale} Il grado di appartenenza, in questo caso, viene fatto decrescere esponenzialmente tramite una funzione esponenziale $e$
\begin{equation}
    p(r)= \begin{cases} 1 & \mbox{se } r \leq r_1, \\e(r) & \mbox{altrimenti}. \end{cases}
\end{equation}

\paragraph{Fuzzificatori a tratti basati su quantili} Sono stati, inoltre, implementati dei fuzzificatori basati su quantili, sia nella variante costante che in quella lineare.
In questo caso gli step della funzione sono definiti in funzione dei quartili delle distanze quadratiche tra le immagini dei punti e il centro della sfera appresa.

\subsection{Induzione di insiemi fuzzy}
Se $F$ è l'insieme fuzzy analizzato, l'algoritmo descritto nel Paragrafo~\ref{mulearn} individua $K$, detto \textit{core}, ovvero l'insieme crisp di tutti i punti che hanno valore unitario per il grado di appartenenza a $F$. Sulla base del fuzzificatore impostato (Paragrafo~\ref{fuzzificatore}), poi, l'algoritmo fa decrescere il valore di appartenenza previsto man mano che i punti si allontanano dal core. 
Al fine di visualizzare il comportamento descritto, si mostra in Figura~\ref{mulearnplot} un esempio di esecuzione dell'algoritmo e la conseguente rappresentazione del risultato ottenuto:
si assuma che le fake news siano appartenenti a un determinato insieme fuzzy $F$ e si supponga di volerlo individuare tramite la procedura vista in precedenza.
Con un campione di 400 notizie (228 vere e 172 fake), opportunamente preprocessato con le tecniche viste nel Paragrafo~\ref{nlp}, è stato eseguito $\mu$-learn con un kernel gaussiano e un fuzzificatore di tipo lineare; successivamente, i risultati sono stati rappresentati in uno spazio bidimensionale applicando la tecnica di PCA~\cite{30}.

Considerando che i punti rossi fanno riferimento alle osservazioni etichettate come fake news e quelli blu alle notizie vere, si osserva la formazione di diverse \textit{curve di livello} che descrivono i gradi di appartenenza a $F$ in prossimità della zona in cui i punti rossi sono maggiormente concentrati.

Man mano che ci si avvicina alle zone periferiche il grado viene diminuito linearmente rispettando i valori delle curve di livello.
\begin{figure}
    \centering
    \includegraphics[scale=0.44]{images/mulearn.png}
    \caption{Visualizzazione in uno spazio bidimensionale dei punti (400 osservazioni) e del fuzzy set associato alle fake news indotto da $\mu$-learn. In rosso le notizie etichettate come ``fake'' e in blu come ``vere''.
    Le curve di livello discriminano i gradi di appartenenza dei punti a partire dall'insieme core al centro fino al resto delle osservazioni nelle zone periferiche.}
    \label{mulearnplot}
\end{figure}
La fase di induzione della funzione di appartenenza è lo step in cui l'algoritmo viene effettivamente addestrato tramite i dati etichettati.

L'esito dell'induzione della funzione di appartenenza è influenzato dalla scelta degli iperparametri, che vengono descritti nel paragrafo successivo.

\subsection{Iperparametri}\label{hyperparameters}
Gli \textit{iperparametri} sono parametri speciali che non vengono appresi direttamente dall'algoritmo di Machine Learning, al contrario, il loro valore viene assegnato prima dell'addestramento.
Nel contesto considerato da questa tesi si distinguono i seguenti iperparametri: 
\begin{enumerate}
    \item il solver utilizzato,
    \item il tipo di kernel,
    \item eventuali parametri che dipendono dal tipo di kernel,
    \item il parametro di regolarizzazione $C$,
    \item il tipo di fuzzificatore,
    \item eventuali parametri legati al tipo di fuzzificatore.
\end{enumerate}
Nella soluzione proposta in questa tesi, fra le svariate possibilità esistenti, è prevista la scelta tra due solver: \textit{tensorflow} e \textit{gurobi}, che usano strategie differenti per il problema di ottimizzazione non lineare. Il secondo e il terzo iperparametro condizionano indirettamente la forma dell'insieme indotto.

Il parametro $C$ impatta sulla dimensione del core dell'insieme:
all'aumentare di $C$, il core si dilata e incrementa il numero di elementi in esso inclusi.
Inoltre, è stato osservato sperimentalmente che, al raggiungimento dell'unità da parte di $C$, l'insieme fuzzy tende a un insieme regolare che racchiude i punti del campione le cui etichette sono diverse da zero.

Infine, il quinto e il sesto iperparametro sono legati al processo di fuzzificazione precedentemente definito nel Paragrafo~\ref{fuzzificatore}.
Una volta individuato un insieme di valori per ogni iperparametro, è possibile ricorrere a una tecnica per la ricerca dei loro valori ottimali, detta \textit{tuning}.

\paragraph{Tuning} Con tuning si intende l'addestramento di uno stimatore con diverse configurazioni dei suoi iperparametri, al fine di valutare i risultati delle predizioni e, sulla base di essi, scegliere opportunamente il set di valori ottimali per la generazione del modello finale.

Nella soluzione proposta in questa tesi, la fase di tuning è stata concretamente implementata tramite la cosiddetta \textit{grid search}: si tratta di una ricerca del modello migliore che considera una tabella di valori che si desidera esplorare e si basa sulla suddivisione del campione dei dati in \textit{training set}, \textit{validation set} e \textit{test set}.
Questi tre tipi di insieme vengono ricavati dal dataset a disposizione mantenendo la composizione denotata nel Paragrafo~\ref{svcmodified}, ovvero formati da coppie di vettori di feature e di etichette. Essi si differenziano tra loro per il diverso scopo al quale ciascuno è dedicato: i dati del training set vengono utilizzati per l'addestramento, il validation set serve a valutare la bontà delle predizioni ed eventualmente regolare gli iperparametri per ottenere risultati migliori. Il test set, invece, serve per testare il modello finale così da trarre le relative conclusioni.
In generale, è importante che ciascuno di questi insiemi, così come il dataset di partenza, siano rappresentativi\footnote{Per dataset rappresentativo si intende un campione sufficientemente popolato e possibilmente bilanciato.} del dominio che si intende considerare.

L'algoritmo viene configurato con ogni combinazione della grid search, generando per ciascuna un modello che viene addestrato col training set, un insieme di dati etichettati necessari per formare la memoria del modello predittivo. Successivamente, il modello osserva i dati del validation set, cioè nuove osservazioni del campione che permettono di fare un confronto con le relative previsioni fatte dal modello indotto. Tali predizioni vengono, dunque, comparate alle etichette originali per misurare la capacità predittiva del modello; questo viene fatto tramite delle apposite metriche e, sulla base di queste, si sceglie il modello effettivamente più efficace.

Il test set, invece, funge da prova finale per valutare la capacità di generalizzazione del modello su dati mai visti, valutando la bontà delle sue predizioni su di essi.
Esistono tanti modi per dividere i dati nei tre insiemi sopra descritti, uno di questi è la \textit{cross validation}.


\paragraph{Cross validation} 
La cross validation è una tecnica che consiste nel partizionare il dataset in $k$ \textit{fold} equiampie, effettuare altrettante iterazioni e, per ciascuna, considerare l'$i$-esima fold come validation set e le restanti $k-1$ come training set per addestrare il modello generato.
In questo caso esistono diverse scuole di pensiero sulla percentuale che si preferisce impostare per la suddivisione in fold.
In generale, è opportuno bilanciare le fold in maniera tale da avere un numero sufficiente di dati per fare l'apprendimento e per fare la validazione.

Indirettamente, definire ogni combinazione delle fold equivale a definire un nuovo modello, pertanto fare una cross validation $k$-fold significa generare $k$ modelli.

Questa tecnica ha il vantaggio di massimizzare l'utilizzo dei dati, utilizzando a turno ogni fold come validation set e, dunque, consentendo di eseguire l'algoritmo su più casi.

In realtà, la stessa tecnica può essere utilizzata per ottenere anche i test set legati alla valutazione della generalizzazione; in questo caso, però, è necessario fare quella che viene definita \textit{cross validation annidata}.

\paragraph{Cross validation annidata}
La cross validation annidata è un'estensione della versione riportata nel precedente capoverso e può essere usata per ricavare diversi test set. Quest'operazione viene fatta eseguendo un'ulteriore cross validation interna per ciascuna delle $k$ fold, ottenendo $l$ fold interne. A questo punto la validazione del modello viene spostata alle fold interne, in cui si opera la ricerca degli iperparametri ottimali, mentre quelle esterne consentono di testare il modello.

\section{Valutazione dei modelli}\label{evaluation}
La scelta dei modelli si basa sull'utilizzo di uno o più criteri di valutazione. Il range di possibilità esistenti in questo senso è molto ampio e la motivazione che porta a preferire una soluzione piuttosto che un'altra dipende strettamente dal tipo di problema considerato, così come dal tipo di dati a disposizione: nell'ambito della classificazione, ad esempio, ha senso ragionare in termini di accuratezza, intesa come la percentuale di predizioni corrette rispetto a quelle totali; in un contesto differente, come ad esempio quello della regressione, non è più possibile basare la bontà di un modello sull'accuratezza, poiché entra in gioco quella che è la quantificazione dell'errore. L'errore è una misura del discostamento che si verifica tra una predizione e la ground truth, ossia l'effettivo valore delle etichette nel dominio di conoscenza considerato.
La differenza è che, mentre un classificatore (binario) può commettere solo due tipi di errore\footnote{Generalizzando, un classificatore $n$-ario può commettere $n(n-1)$ diversi tipi di errore.}, denominati \textit{falsi positivi} e \textit{falsi negativi}, nel caso della regressione l'errore può essere un qualunque numero positivo.
In generale, ricorrere a queste misure consente di descrivere quanto sia rilevante l'errore compiuto dal predittore e stabilire se sia tollerabile per la scelta finale del modello.

\subsection{Matrice di confusione}
Quando si mira ad ottenere un classificatore, uno strumento molto utilizzato per valutare la sua bontà è la matrice di confusione, una struttura dati che analizza le predizioni e le divide nei possibili esiti. 
Il caso più semplice è quello mostrato in Figura~\ref{confusion} in cui si considera delle etichette binarie: dati due valori di risposta da parte del classificatore e due valori di verità della ground truth, si ottengono due tipi di predizione corretta (veri positivi e veri negativi) e due casi di predizione sbagliata (falsi positivi e falsi negativi).

Nel contesto di classificazioni multi-classe è sufficiente estendere la matrice ottenendo sulla diagonale il numero di predizioni corrette e nel resto della matrice i vari casi di errore con cui è possibile capire quali classi vengano più o meno facilmente confuse. 

\begin{figure}
    \centering
    \includegraphics[scale=0.7]{images/confusion_matrix.png}
    \caption{Matrice di confusione su classificazione binaria. Nell'ambito considerato in questa tesi la classificazione ``positiva" riguarda il fatto che una notizia sia fake, dualmente quella ``negativa" si riferisce alle notizie vere.}
    \label{confusion}
\end{figure}

\subsection{Precision, Recall e F1}\label{erroreclass}
L'accuratezza è una misura utile ma che assume una distribuzione uniforme delle quattro classi individuate nella matrice di confusione. Nel caso in cui tale distribuzione non sia effettivamente uniforme, questo tipo di misura può risultare ingannevole: si consideri, ad esempio, un campione di 1000 predizioni in cui si ha 1 vero positivo, 0 falsi positivi, 1 falso negativo e 998 veri negativi. In questo caso si otterrebbe un'accuratezza pari al 99.9\% ma, come è facile intuire da questo esempio giocattolo, tale misura non fornisce alcuna informazione riguardo a come le predizioni corrette siano distribuite.
Nel contesto della classificazione è possibile, infatti, calcolare delle misure quantitative per valutare la bontà di un predittore che tengono in considerazione la distribuzione delle predizioni; esse sono dette \textit{Precision}, \textit{Recall} e \textit{F1} e sono basate sul numero di veri positivi, veri negativi, falsi positivi e falsi negativi.

\paragraph{Precision}
Il valore di \textit{Precision} misura la frazione di predizioni positive correttamente individuate dal classificatore. Nell'esempio delle fake news si potrebbe interpretare questa misura come la percentuale di notizie classificate come fake che effettivamente sono false.
\begin{equation}
\frac{\text{Veri Positivi}}{\text{Veri Positivi} + \text{Falsi Positivi}}
\end{equation}

\paragraph{Recall}
La \textit{Recall} quantifica il numero di predizioni positive corrette rispetto a tutte le osservazioni positive. Ricollegandosi alle fake news, la Recall rappresenterebbe la percentuale di tutte le notizie fake del dataset che sono state individuate dal classificatore.
\begin{equation}
\frac{\text{Veri Positivi}}{\text{Veri Positivi} + \text{Falsi Negativi}}
\end{equation}

\paragraph{F1}
Dal momento che le precedenti quantità rappresentano due facce della stessa medaglia, il punteggio \textit{F1} nasce con l'intento di combinarle tramite la media armonica.
\begin{equation}
\frac{2 \times (\text{Precision} \times \text{Recall})}{\text{Precision} + \text{Recall}}
\end{equation}

\subsection{MSE e RMSE}\label{erroreregr}
Il concetto di accuratezza di un modello non è sempre applicabile, specialmente se si considera un tipo di predittore che produce valori continui\footnote{Qui il termine ``continui'' è da intendersi da un punto di vista puramente teorico, in quanto a livello informatico si tratta comunque di un intervallo di valori discreto, derivante dalla limitata capacità di memoria di un calcolatore.}. In questi casi si ricorre a delle misure del concetto di \textit{errore}, ossia la quantità che descrive quanto il valore della predizione sia deviato rispetto alla ground truth.
In questa tesi sono state utilizzate le metriche di \textit{mean squared error} e di \textit{root mean squared error}.

Nelle definizioni che seguono, $S$ denota il dataset formato da tutte le coppie $(\mathbf{x_i},y_i)$ con $i=1, ..., m$,
dove $\mathbf{x_i}$ indica il vettore delle feature, $y_i$ l'etichetta, $i$ l'indice di scorrimento delle osservazioni e $m$ è la dimensione del dataset.
\paragraph{MSE} L'errore quadratico medio è definito come
\begin{equation}
    \frac{1}{m}\sum\limits_{j=1}^m (\hat{y_j} - y_j)^2
\end{equation}
dove $\hat{y}_j$ è la predizione.
\paragraph{RMSE}
Al fine di portare il valore dell'errore ad avere la stessa unità di misura delle predizioni può essere utile sfruttare l'RMSE, definito come 
\begin{equation}
    \sqrt{MSE}
\end{equation}

\section{Bontà di generalizzazione di un modello}\label{generalization}
Nell'ambito del Machine Learning, generare un modello predittivo affidabile non significa selezionare semplicemente il modello che ottiene i risultati migliori bensì quello che si adatta meglio ai dati.
Questa capacità di adattamento consente di valutare quella che viene definita la \textit{bontà di generalizzazione} che il modello ha sui dati che non ha mai visto. Da questo punto di vista, un modello con una buona capacità di generalizzazione è in grado di intuire correttamente le etichette dei dati perché è riuscito ad estrarre efficacemente la relazione di dipendenza tra le feature e le etichette target desiderate.
A volte, questo meccanismo viene raggiunto sui dati di addestramento tramite una continua regolazione degli iperparametri nella fase di tuning, tuttavia questo rischia di causare la generazione di un modello che su quel tipo di dato riesce a ottenere buoni risultati ma che ha una bassa capacità di generalizzazione su altri poiché, più che formare la sua esperienza su concrete relazioni dei dati, ha raggiunto l'obiettivo tramite un eccessivo incremento della complessità.

Gli iperparametri determinano, infatti, i gradi di libertà del modello ottenuto e la loro numerosità ha un impatto sulla complessità del modello: avere molti iperparametri rischia di complicarlo eccessivamente, dualmente, averne pochi, lo rende potenzialmente troppo semplice. 
Sulla base di queste premesse si introducono, quindi, i concetti di \textit{overfitting} e \textit{underfitting}.

\subsection{Overfitting e underfitting}
Si distinguono modelli che soffrono di \textit{underfitting} quando hanno errore alto sul training set e sul test set, tipicamente questo è causato da un'eccessiva semplificazione del modello o da una quantità di dati di addestramento insufficiente.

Al contrario, modelli che soffrono di \textit{overfitting} sono troppo complessi e mostrano un errore basso sul training set ma alto sul test set, evidenziando una scarsa capacità di generalizzazione.

In letteratura, il problema è descrivibile anche in termini di equilibrio tra \textit{bias} e \textit{variance error}, derivanti dalla \textit{decomposizione bias-variance}; tale decomposizione descrive le componenti che formano l'errore totale di uno stimatore prodotto da qualsiasi algoritmo di Machine Learning: bias error, variance error e Bayes error.

Il bias error è l'errore costituito dalle ipotesi semplificative fatte da un modello per rendere più facile l'apprendimento della variabile target.

Il variance error è, invece, la quantità che misura quanto cambia la previsione della variabile target al variare dei dati di addestramento.

Infine, il Bayes error è una quantità irriducibile e, dal punto di vista dell'apprendimento automatico, inevitabile; si tratta dell'intrinseco errore introdotto dalla scelta del problema.

Da questo punto di vista, l'underfitting è il risultato del prevalere del bias error sul variance error; l'overfitting, invece, è causato dal dominare del variance error sul bias error.
Di conseguenza, al fine di trovare un compromesso per controllare questo tipo di problematica, è necessario avere una giusta complessità del modello, associata a un campione di dati sufficientemente grande.

In Figura~\ref{underfittingoverfitting} viene riportato un confronto tra due modelli: nell'immagine di sinistra è presente un modello troppo semplice con un eccessivo bias error; a destra, invece, un modello con tanti gradi di libertà allo scopo di adattarsi molto bene ai punti ma che porta ad aumentare eccessivamente il variance error, rivelandosi poco robusto su nuove osservazioni. 
Un altro aspetto che non dipende dalla complessità del modello e a cui, però, occorre prestare comunque attenzione è il fenomeno di \textit{data leakage}.

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/underfitting.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/overfitting.png}
    \end{minipage}
    \caption{Nel primo grafico si mostra un esempio di modello di Machine Learning che evidenzia problemi di underfitting; nel secondo caso si mostra un tipico comportamento di overfitting - Fonte: \url{https://towardsdatascience.com/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690}.}
    \label{underfittingoverfitting}
\end{figure} 

\paragraph{Data leakage} Per \textit{data leakage} si intende lo scenario in cui il training set o il validation set vengono contaminati dal test set. In altre parole violare la proprietà di disgiunzione tra questi due tipi di insiemi causa un eccessivo adattamento del modello sui dati di allenamento e, di conseguenza, esso performa molto bene su questi ultimi ma molto male su osservazioni nuove.
Questa è la ragione per cui il partizionamento del campione in training, validation e test set deve essere fatto rendendo tali insiemi disgiunti fra loro.

\section{Il sistema}
\label{sistema}
In questo paragrafo si mostra ad alto livello l'architettura della soluzione proposta in questa tesi per la rilevazione delle fake news tramite l'induzione di insiemi fuzzy, presentando prima i nodi che la compongono e poi descrivendo come sia stato utilizzato l'algoritmo $\mu$-learn nel contesto del riconoscimento delle fake news.
Nella seconda parte viene, invece, spiegato come i nodi in questione comunichino fra loro e quali file vengano generati durante l'utilizzo del sistema. Sarà, quindi, obiettivo del Capitolo~\ref{Capitolo 3} illustrare più a basso livello l'implementazione del sistema e descrivere nel dettaglio i nodi che vi appartengono.

Per gestire la pluralità dei casi nella forma dell'input, il sistema definisce un formato standard per i dataset che andrà a trattare: il dataset ideale è un dataframe che contiene in forma tabulare osservazioni relative a dati testuali che, nell'ambito trattato da questa tesi, possono essere il corpo delle notizie o i titoli.
Tali osservazioni sono etichettate con valore 1 se si tratta di una notizia fake, 0 altrimenti.

L'obiettivo di $\mu$-learn sarà, quindi, indurre l'insieme fuzzy delle fake news inizialmente sconosciuto ed apprendere la funzione di appartenenza ad esso associata.
\subsection{Architettura}\label{architecture}
L'architettura del sistema è composta da tre nodi, dedicati alle seguenti attività: 
\begin{enumerate}
    \item il preprocessing del dataset,
    \item la selezione dei modelli migliori,
    \item la visualizzazione dei dati.
\end{enumerate}
Segue, quindi, una breve descrizione di ciascun nodo più nel dettaglio.

\paragraph{Preprocessing del dataset} All'inizio di questa fase ci sono tre possibili scenari: i) il dataset rispetta il formato predefinito, ii) il dataset è formattato diversamente e dunque necessita di una fase di rielaborazione per poter essere utilizzabile, iii) il dataset viene generato dal sistema stesso.
In altre parole, nel secondo caso è necessario rendere la forma dell'input compatibile col formato predefinito; questo viene fatto da un modulo che ha la funzione di estrarre le informazioni fondamentali e di formare con esse il dataframe desiderato.
A questo punto interviene la vera e propria fase di preprocessing che consiste nell'eseguire una pipeline di operazioni che hanno l'obiettivo di preparare il dataset alla fase successiva.

Le operazioni in questione includono le tecniche di elaborazione del linguaggio naturale descritte nel Capitolo~\ref{Capitolo 1} e la cui implementazione viene illustrata nel dettaglio nel Capitolo~\ref{Capitolo 3}.

La pipeline che assembla tali operazioni e che le esegue in sequenza viene opportunamente configurata a seconda delle esigenze per attivare o disattivare gli step per l'elaborazione dei dati, anche in base allo scenario considerato.

Al termine del preprocessing, un nuovo dataset viene creato e scritto su file, per il suo utilizzo in un secondo momento. L'intero processo viene schematizzato in Figura~\ref{preprocessmodule}.

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/preprocessingmodule.png}
    \caption{Primo nodo dell'architettura, dedicato al preprocessing dei dati, secondo i tre possibili scenari.}
    \label{preprocessmodule}
\end{figure}

\paragraph{Selezione dei modelli}
La fase di selezione dei modelli si occupa di individuare il modello ideale tenendo presente le considerazioni fatte nei Paragrafi~\ref{evaluation} e~\ref{generalization}, relative alla valutazione della capacità predittiva e alla bontà di generalizzazione dei modelli.

Al fine di rendere computazionalmente più agevole l'esecuzione degli esperimenti si è optato per isolare ciascun nodo per permettere, ad ogni step, la scrittura in memoria persistente dell'output. Nel caso della model selection risulta particolarmente conveniente disporre di un dataset precedentemente elaborato su cui impostare la selezione dei modelli in quanto, innanzitutto potrebbe richiedere notevole tempo per terminare, e soprattutto lo stesso campione elaborato potrebbe essere utilizzato più volte.

I modelli ottimali vengono generati tramite la tecnica della cross validation annidata e, per ciascuna iterazione, viene fatta una \textit{grid search} per indagare quali siano i migliori modelli sulla base del validation error ottenuto.

Alla fine di quest'operazione vengono ricavati, infatti, i modelli che si sono adattati meglio ai dati e fornita la relativa configurazione degli iperparametri.

\`E in questa fase che si colloca l'utilizzo di $\mu$-learn in quanto le configurazioni della grid search riguardano gli iperparametri di tale algoritmo.
I modelli selezionati vengono, dunque, serializzati per permettere il loro riutilizzo in un secondo momento; più precisamente, questo avviene nel terzo nodo di visualizzazione dei dati in cui si vuole valutare il loro comportamento.

In Figura~\ref{selectionmodel} viene mostrata la rappresentazione del funzionamento sopra descritto.
\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/modelselectionmodule.png}
    \caption{Secondo nodo dell'architettura, dedicato alla selezione dei modelli.}
    \label{selectionmodel}
\end{figure}

\paragraph{Visualizzazione dei dati} 
Nella parte di visualizzazione dei dati, i modelli vengono deserializzati per poterli riutilizzare al fine di operare un'analisi descrittiva.
Tale analisi coinvolge principalmente i gradi di appartenenza approssimati dai modelli generati dal sistema, valutando la bontà delle predizioni da essi effettuate.

Il criterio con cui viene valutata la correttezza delle predizioni consiste nell'utilizzare le misure quantitative descritte nei Paragrafi~\ref{erroreclass} e~\ref{erroreregr}. Si sottolinea, inoltre, che in questa fase vengono effettuati dei confronti con una determinata \textit{baseline}, ovvero uno o più stimatori di riferimento le cui predizioni vengono valutate sugli stessi dati trattati da $\mu$-learn, per poter fornire un ulteriore metro di paragone. Come mostrato in Figura~\ref{datavisualization}, l'analisi può interagire nuovamente col dataset di partenza per poter visionare quelle osservazioni che causano incertezza nelle predizioni. Complessivamente, il terzo nodo è importante per poter estrarre maggior conoscenza dall'ambito preso in considerazione. Tutte le conclusioni tratte dall'analisi descrittiva, infatti, vengono riassunte in un report e, sulla base di esso, si può eventualmente decidere se ricominciare gli esperimenti.

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/datavisualizationmodule.png}
    \caption{Terzo nodo dell'architettura, dedicato alla visualizzazione dei dati.}
    \label{datavisualization}
\end{figure}

\subsection{Funzionamento del sistema}
In questo paragrafo si descrive come funziona il sistema nella sua complessità e come esso sia stato utilizzato per produrre dei risultati.
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.6]{images/cycle.png}
    \caption{Schema del flusso di comunicazione all'interno del sistema.}
    \label{cycle}
\end{figure}
In Figura~\ref{cycle} viene raffigurato il flusso di comunicazione all'interno del sistema, numerato passo per passo: 
\begin{enumerate}
    \item il processo ha inizio quando si dispone di un dataset, ottenuto in uno dei modi visti in precedenza, e viene così acquisito dal primo nodo per il preprocessing;
    \item  il nodo effettua il preprocessing del dataset e ne produce uno nuovo che viene opportunamente scritto su file per permettere la successiva lettura da parte del nodo di model selection;
    \item i modelli predittivi ottimali sono stati trovati, serializzati e il sistema aggiorna la storia degli esperimenti, documentata in un registro dedicato. Lo scopo del registro è catalogare i modelli e i loro dati associati tramite l'informazione di data e ora;
    \item i modelli vengono deserializzati e usati per l'analisi descrittiva delle predizioni;
    \item tramite il registro, il sistema è in grado di risalire al dataset utilizzato per ottenere i relativi modelli e, dunque, completare l'analisi;
    \item il modulo per la visualizzazione dei dati fornisce un report che viene aggiunto a quelli associati agli altri esperimenti;
    \item il report permette di analizzare il comportamento dei modelli e individuare delle possibili configurazioni da testare per i nuovi esperimenti;
    \item se si desidera fare nuovi esperimenti
    
    a) ripartendo dal preprocessing dei dati, è necessario ricominciare l'esperimento dall'inizio, andando a modificare i parametri in questione;
    
    b) modificando il modo in cui viene fatta la grid search, si regolano i relativi valori a partire dalla model selection.
\end{enumerate}

\section{Tipi di predittore considerati}\label{predictors}
Le considerazioni fatte nei Paragrafi~\ref{erroreclass} e~\ref{erroreregr} portano a domandarsi quale tipo di predittore sia il caso di considerare per il problema delle fake news.

Da questo punto di vista sono state esplorate due possibilità: la pura induzione della funzione di appartenenza e un classificatore binario derivato da tale induzione.
Siano $\lambda$ e $\omega$ i due predittori appena presentati, si definisce il primo predittore come
\begin{equation}
    \lambda: \mathbb{R}^h \rightarrow [0,1]
\end{equation}
dove $h$ è il numero di feature; sia $x$ una generica osservazione del dataset, il secondo predittore $w: \mathbb R^h \rightarrow \{ 0, 1 \}$ è definito come
\begin{equation}
    \omega(x) = \begin{cases} 1 & \mbox{se } \lambda(x) \geq 0.5, \\ 0 & \mbox{altrimenti.} \end{cases}
\end{equation}
Ne consegue che per $\lambda$ risultano più appropriate le misure di MSE e RMSE, mentre per $\omega$ sono state scelte le metriche di Precision, Recall e F1\footnote{In realtà si tratta di una versione leggermente modificata: si tratta di calcolare le misure di Precision, Recall e F1 pesando il numero di veri positivi e falsi positivi con $\mu$ e il numero di falsi negativi per $1-\mu$.}.

\chapter{Implementazione}
\label{Capitolo 3}
\onehalfspacing
Questo capitolo tratta l'implementazione della soluzione proposta nel capitolo precedente. Nel Paragrafo~\ref{datasethandle} si descrive la parte inerente alle modalità con cui si gestiscono i vari scenari relativi al tipo di dataset che si ha a disposizione.

All'interno del Paragrafo~\ref{pp} viene illustrata la pipeline di preprocessing, soffermandosi su ognuno degli step che la compongono; il Paragrafo~\ref{modelselection}, invece, descrive come è stata implementata la parte di model selection.
Infine, nel Paragrafo~\ref{datavisualizationimpl} viene descritta la fase di visualizzazione dei dati.

\section{Gestione dei dataset}\label{datasethandle}
Il sistema di apprendimento supervisionato proposto in questa tesi ha bisogno di molti esempi per affinare le predizioni, pertanto sono stati previsti più scenari al fine di massimizzare la mole di dati destinata all'addestramento. Principalmente, le vie percorse sono state due: generare dei dataset e utilizzare dei dataset acquisiti da terzi.
Nel primo caso è stato implementato un modulo adibito alla generazione artificiale di documenti testuali, mentre nel secondo servizi come \textit{Kaggle} hanno facilitato l'individuazione di dataset interessanti. Difficilmente, però, essi sono strutturati nello stesso modo; per questo motivo, come accennato nel Paragrafo~\ref{sistema}, il sistema definisce un formato standard su cui lavorare e, nel caso in cui il dataset considerato non lo rispetti, è possibile eseguire un apposito modulo di adattamento del dataset.

\subsection{Generatore di dataset}\label{generator}
Idealmente, per il problema di apprendimento supervisionato proposto in questa tesi sarebbe necessario considerare dei dati etichettati con il grado di appartenenza all'insieme fuzzy delle fake news. Un tipo di dataset così organizzato è sostanzialmente difficile da reperire, in quanto in letteratura il fenomeno delle fake news viene tipicamente modellato come un problema di classificazione; di conseguenza, al fine di produrre degli esperimenti più esaustivi, si è deciso di implementare un modulo per la generazione di dataset con il grado di appartenenza. Il modulo in questione si basa sulla tecnica di \textit{Latent Dirichlet Allocation} (\textit{LDA} \cite{20}) per la creazione di documenti testuali come misture di parole appartenenti a topic diversi. 
Si denoti, quindi, $t_1$ il topic delle notizie vere e $t_2$ quello delle notizie fake; ciascun topic è rappresentato da un insieme di parole $P_i$ che lo caratterizza. Per semplicità, è stato considerato prima il caso di $P_1  \cap P_2 = \emptyset$ in cui gli insiemi delle parole sono disgiunti, e poi quello dove la loro intersezione non è vuota.

La tecnica utilizzata prevede la generazione di $m$ documenti composti da $n$ parole estratte dai topic nel modo seguente: 
\begin{itemize}
    \item si fissa l'insieme $P_2$, rappresentativo del topic $t_2$ delle fake news;
    \item si definisce grado di appartenenza ``atteso'' dell'$i$-esimo documento la probabilità di successo $\mu_i$ di una distribuzione bernoulliana che una parola $p_j \in P_2$ venga inserita nel documento, con $i=1, ..., m$ e $j=1, ..., n$;
    \item si indicano con $B$ e $Z$ due variabili casuali che seguono rispettivamente la distribuzione bernoulliana con parametro uguale a $\mu_i$ e la distribuzione di Zipf \cite{34};
    \item si fissa un'arbitraria distribuzione di probabilità $d$ con supporto $[0, 1]$, così da utilizzare le sue specificazioni come valori per $\mu_i$;
    \item si esegue l'Algoritmo~\ref{lda}.
\end{itemize}  

\begin{algorithm}
\caption{procedura del \texttt{generatore di dataset}}
\label{lda}
\hspace*{\algorithmicindent} \textbf{Procedure} generator($m$, $n$, $d$, $P$)
\newline
\hspace*{\algorithmicindent} \textbf{Input}: $m$ è il numero di documenti, $n$ il numero di parole per ogni documento, $d$ è la distribuzione arbitraria, $P$ è la lista che contiene $P_1$ e $P_2$: gli insiemi di parole che rappresentano i due topic
\newline
\hspace*{\algorithmicindent} \textbf{Output}: $m$ documenti testuali artificiali
\begin{algorithmic}[1]
\STATE documents = [ ]
\FOR{$i$ in $1, ..., m$}
\STATE $\mu_i$ = extraction($d$)
\STATE document = [ ]
\STATE document.membership = $\mu_i$
\FOR{$j$ in $1, ..., n$}
\STATE $B$ = extraction(Bernoulli, probability = $\mu_i$)
\STATE $Z$ = extraction(Zipf)
\STATE word = $P$.index($B, Z$)
\STATE document.add(word)
\ENDFOR
\STATE documents.add(document)
\ENDFOR
\RETURN documents
\end{algorithmic}
\end{algorithm}
In pratica, si tratta di un processo non deterministico di estrazione delle parole, in cui la variabile $Z$ indica la parola estratta e $B$ è la variabile casuale bernoulliana che determina il topic selezionato in base alla probabilità di successo $\mu_i$. Nel caso di due topic si hanno due specificazioni possibili: 0 per il topic delle notizie vere e 1 per le fake news, allora si definisce di ``successo'' l'evento $B=1$ con probabilità $\mu_i$ e l'evento di ``insuccesso'' $B=0$ con probabilità $1-\mu_i$, o equivalentemente
\begin{equation}
    \mathsf{P}(p_j \in P_2) = \mu_i,
\end{equation}
\begin{equation}
    \mathsf{P}(p_j \in P_1) = 1 - \mu_i.
\end{equation}
L'obiettivo del generatore è misurare la capacità predittiva dell'algoritmo $\mu$-learn e capire la potenzialità di questo approccio nell'avvicinarsi al grado di appartenenza atteso. Tramite questo approccio si valutano, quindi, le performance dell'algoritmo di apprendimento quando questo elabora un dataset con etichette non binarie.

\subsection{Adattatore di dataset}\label{adapter}
Per gestire la diversità della forma dei dati acquisiti da terzi, si è deciso di fissare un formato standard e, in base ad esso, adattare o meno il dataset a disposizione in modo che rispetti tale convenzione. L'obiettivo è facilitare l'esecuzione del sistema descritto nel Paragrafo~\ref{sistema} su nuovi dati; pertanto, tale sistema assume di lavorare con un dataframe contenente tre colonne: \textit{index}, \textit{text} e \textit{label}.

L'indice ha la finalità di preservare l'identificatore di ogni osservazione, questo torna particolarmente utile i) quando si vogliono analizzare specifiche notizie durante la fase di data visualization e ii) per mantenere il riferimento alla posizione originale delle notizie rimescolate durante lo step di shuffling nel preprocessing.

Il testo riguarda delle possibili sequenze di parole che, nell'ambito delle fake news, possono essere il corpo delle notizie, il titolo o, a seconda del dataset considerato, il contenuto di possibili tweet o post sui social media.

L'etichetta ha due forme possibili: un valore binario per i dataset recuperati da Kaggle o un valore compreso nell'intervallo $[0,1]$ per il caso di dataset generati.

\section{Pipeline di preprocessing}\label{pp}
Per la fase di preprocessing è stata predisposta una pipeline, ovvero un insieme di operazioni eseguite consecutivamente.
La sequenza di operazioni viene specificata al momento dell'esecuzione tramite un'opportuna parametrizzazione degli step della pipeline.

In generale, l'interfaccia richiesta dalla libreria \textit{scikit-learn} per l'implementazione di nuovi stimatori prevede di separare le procedure di \textit{fit}, \textit{predict} e \textit{transform}: esse definiscono rispettivamente la parte di apprendimento dei dati, quella di predizione e quella di effettiva trasformazione dell'input in un output elaborato. Nel caso del preprocessing, la maggior parte delle operazioni della pipeline è caratterizzata da una procedura di transform, tuttavia alcune, come Word2Vec, implementano anche la funzione fit in quanto possono richiedere una fase di addestramento.
Adottare questa convenzione rappresenta un vantaggio per poter comodamente accedere a funzioni di libreria ad alto livello per la model selection e per la valutazione dei modelli: la funzione \textit{GridSearchCV}, ad esempio, può essere utilizzata specificando direttamente come parametro lo stimatore derivato dall'algoritmo $\mu$-learn, in quanto quest'ultimo è stato definito nel modo visto sopra.
Inoltre, il codice organizzato in questa maniera si presta bene a future estensioni e favorisce la sua manutenibilità poiché rispetta il criterio della \textit{Separation of Concerns}, concetto cardine nell'ambito dell'Ingegneria del software \cite{32}.

Quella che segue è la lista dei passi della pipeline che al momento della configurazione è possibile attivare a seconda delle esigenze:
\begin{itemize}
    \item lowercase,
    \item rimozione dei duplicati,
    \item lemmatizzazione,
    \item tokenizzazione,
    \item rimozione del rumore,
    \item stemming,
    \item rimozione delle stop word,
    \item word2vec,
    \item aggregazione,
    \item doc2vec.
\end{itemize}
Addizionalmente si considera anche uno step di shuffling che applica una permutazione dei documenti e uno finale di standardizzazione dei valori delle feature.

In generale, le operazioni della pipeline non sono tutte compatibili; al contrario, alcune sono tra di loro esclusive, come l'uso di Word2Vec e di Doc2Vec, oppure la lemmatizzazione e lo stemming.
Inoltre, l'ordine con cui esse vengono applicate è determinante per la rielaborazione del testo: è più semplice rimuovere il rumore analizzando i token piuttosto che le stringhe di intere frasi, ecco perché la tokenizzazione avviene tipicamente nei primi step della pipeline. Un altro esempio è quello dell'aggregazione che, come visto nei Paragrafi~\ref{w2v} e~\ref{d2v}, è necessaria solo in caso di utilizzo di Word2Vec.
\begin{figure}
    \centering
    \includegraphics[scale=0.395]{images/pipeline.png}
    \caption{Esempio di configurazione di una pipeline di preprocessing.}
    \label{pipeline}
\end{figure}
In Figura~\ref{pipeline} viene mostrato un esempio di pipeline che è stata frequentemente utilizzata durante gli esperimenti. Si noti come in questo tipo di configurazione non sia necessario includere lo step di tokenizzazione in quanto la lemmatizzazione prevede una suddivisione in token che garantisce lo stesso risultato.
Nei paragrafi successivi si descrivono brevemente i possibili passi della pipeline; per ciascuno di essi viene fornito un esempio mostrando un input semplificato e il corrispondente output.

\paragraph{Lowercasing}
Il \textit{lowercasing} è una tecnica che consiste nel trasformare una stringa di testo in una stringa composta unicamente da caratteri minuscoli.
\\
\\
\textbf{Input}: dataframe di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & `\textit{This is a sentence. Lowercasing is going to do its job.}'\\
         1 & `\textit{This is another sentence.}'\\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & `\textit{this is a sentence. lowercasing is going to do its job.}'\\
         1 & `\textit{this is another sentence.}'\\
    \hline
    \end{tabular}
\end{center}

\paragraph{Rimozione dei duplicati}
In questo step vengono rimossi i duplicati presenti nel dataset, intesi come le osservazioni ripetute nel dataset.
\\
\\
\textbf{Input}: dataframe di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & `\textit{This is a duplicate sentence.}'\\
         1 & `\textit{This is a duplicate sentence.}'\\
         2 & `\textit{This is another sentence.}'\\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & `\textit{This is a duplicate sentence.}'\\
         1 & `\textit{This is another sentence.}'\\
    \hline
    \end{tabular}
\end{center}

\paragraph{Lemmatizzazione}
La \textit{lemmatizzazione} è la tecnica che ricava da una frase i lemmi che la compongono sulla base della morfologia delle parole.
Questo step include implicitamente la divisione in token.
Si osservi, inoltre, che nel caso di pronomi la lemmatizzazione produce una generica stringa \texttt{-PRON-} in quanto morfologicamente non esiste una base.
\\
\\
\textbf{Input}: dataframe di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & `\textit{this is a sentence that is going to be split.}'\\
         1 & `\textit{here another sentence which confirms it.}'\\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe di liste di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [\textit{`this', `be', `a', `sentence', `that', `be', `go', `to', `be', `split', `.'}]\\
         1 & [\textit{`here', `another', `sentence', `which', `confirm', `-PRON-', `.'}]\\
    \hline
    \end{tabular}
\end{center}

\paragraph{Tokenizzazione}
La \textit{tokenizzazione} consiste nella pura suddivisione delle frasi in token. Per token si intende una sequenza di caratteri raggruppati in modo da formare un significato, tipicamente si tratta di parole.
\\
\\
\textbf{Input}: dataframe di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & `\textit{This is a sentence that is going to be split.}'\\
         1 & `\textit{Here there is another sentence which confirms it.}'\\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe di liste di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [\textit{`This', `is', `a', `sentence', `that', `is', `going', `to', `be', `split', `.'}]\\
         1 & [\textit{`Here', `there', `is', `another', `sentence', `which', `confirms', `it', `.'}]\\
    \hline
    \end{tabular}
\end{center}

\paragraph{Rimozione del rumore}
La \textit{rimozione del rumore} è, a sua volta, scomponibile in molteplici passaggi, ciascuno delegato alla gestione di una specifica categoria di rumore.
In tal senso, sono stati implementate le rimozioni di URL, emoji, punteggiatura, numeri, parole contenenti numeri, parole duplicate, e osservazioni vuote nel dataset. 
\\
\\
\textbf{Input}: dataframe di liste di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [\textit{`This', `is', `www.unimi.it', `\smiley{}'}, `,', `my', `number', `is', `928792', `.']\\
         1 & [\textit{ }] \\
         2 & [\textit{`My', `username', `is', `fire95'}] \\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe di liste di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [\textit{`This', `is', `my', `number'}]\\
         1 & [\textit{`My', `username', `is'}] \\
    \hline
    \end{tabular}
\end{center}

\paragraph{Stemming}
Lo \textit{stemming} ha l'obiettivo di rimuovere suffissi e prefissi per individuare la radice delle parole.
\\
\\
\textbf{Input}: dataframe di liste di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [\textit{`this', `is', `a', `sentence', `that', `is', `going', `to', `be', `split', `.'}]\\
         1 & [\textit{`here', `there', `is', `another', `sentence', `which', `confirms', `it', `.'}]\\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe di liste di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [\textit{`thi', `is', `a', `sentenc', `that', `is', `go', `to', `be', `split', `.'}]\\
         1 & [\textit{`here', `there', `is', `anoth', `sentenc', `which', `confirm', `it', `.'}]\\
    \hline
    \end{tabular}
\end{center}

\paragraph{Rimozione delle stop word}
Come accennato nel Capitolo~\ref{Capitolo 1}, si può scegliere di rimuovere le \textit{stop word}, parole molto frequenti all'interno di un documento ma che poco aggiungono alla semantica del testo.
\\
\\
\textbf{Input}: dataframe di liste di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [\textit{`this', `is', `a', `sentence', `with', `stop', `words'}]\\
         1 & [\textit{`here', `there', `is', `another', `interesting', `case'}]\\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe di liste di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [\textit{`sentence', `stop', `word'}]\\
         1 & [\textit{`interesting', `case'}]\\
    \hline
    \end{tabular}
\end{center}

\paragraph{Word2Vec}
\textit{Word2Vec} è la prima delle due tecniche di embedding citate in questa tesi che converte ogni parola in un vettore di feature numeriche.
\\
\\
\textbf{Input}: dataframe di liste di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [\textit{`thi', `sentenc', `split'}]\\
         1 & [\textit{`anoth', `sentenc', `confirm'}]\\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe di liste di liste di valori float (l'esempio considera 96 feature).
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [[1.12, ..., -3.6],[-1.83, ..., -2.20],[-0.96, ..., -1.04]] (3 liste di 96 elementi) \\
         1 & [[-0.32, ..., -1.96],[-1.83, ..., -2.20],[0.20, ..., -2.60]] (3 liste di 96 elementi)\\
    \hline
    \end{tabular}
\end{center}

\paragraph{Aggregazione}
La fase di \textit{aggregazione} è necessaria nel caso si utilizzi Word2Vec, poiché l'obiettivo finale è ottenere nuovamente un dataframe di vettori. Tra le possibili tecniche di aggregazione presentate alla fine del Paragrafo~\ref{embedding}, in questo lavoro di tesi è stata implementata quella che utilizza la media dei vettori.
\\
\\
\textbf{Input}: dataframe di liste di liste di valori float.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [[1.12, ..., -3.60],[-1.83, ..., -2.20],[-0.96, ..., -1.04]] (3 liste di 96 elementi) \\
         1 & [[-0.32, ..., -1.96],[-1.83, ..., -2.20],[0.20, ..., -2.60]] (3 liste di 96 elementi) \\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe di liste di valori float.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & [-0.81, -0.63, ..., -2.24] (96 elementi) \\
         1 & [-0.88, -1.22, ..., -2.22] (96 elementi) \\
    \hline
    \end{tabular}
\end{center}

\paragraph{Doc2Vec}
\textit{Doc2Vec} è una soluzione alternativa di embedding per convertire direttamente le frasi in vettori; essa non necessita, infatti, di una fase di aggregazione ma richiede come input un dataframe di stringhe.
\\
\\
\textbf{Input}: dataframe di stringhe.
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{index} & \textbf{text} \\
    \hline
         0 & \textit{`thi sentenc split'}\\
         1 & \textit{`anoth sentenc confirm'}\\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe di liste di float (l'esempio considera un modello di 20 feature).
\begin{center}
    \begin{tabular}{|c|c|}
    \hline
         0 & [0.02, 0.03, ..., 0.04] (20 elementi) \\
         1 & [-0.01, -0.03, ..., 0.03] (20 elementi) \\
    \hline
    \end{tabular}
\end{center}

\paragraph{Shuffling}
Fare lo \textit{shuffling} dei dati significa applicare una permutazione delle osservazioni per assicurarsi che il dataset non segua uno specifico ordine. Quando questo accade, infatti, il partizionamento in training, validation e test set (o in generiche fold) rischia di essere fortemente influenzato dal modo in cui i dati sono collocati e di determinare degli insiemi poco rappresentativi del problema che si vuole modellare: si supponga, ad esempio, di voler generare un classificatore di un insieme di animali e che i dati siano organizzati in ordine alfabetico. Nel dataset, quindi, saranno presenti prima tutti i cani, poi tutti i gatti, poi i leoni, e via discorrendo. Nel contesto della cross validation, ad esempio, rimescolare i dati significa evitare che si creino fold composte interamente, o quasi, solo da cani, solo da gatti o leoni. Questo fenomeno prende il nome di \textit{bias dei dati}.
\\
\\
\textbf{Input}: dataframe preprocessato.
\begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{index} & \textbf{text} & \textbf{label} \\
    \hline
         0 & [0.02, 0.03, ..., 0.04] (20 elementi) & 1 \\
         1 & [-0.01, -0.03, ..., 0.03] (20 elementi) & 1 \\
         2 & [0.04, 0.01, ..., 0.91] (20 elementi) & 0 \\
         3 & [-0.07, 0.31, ..., 0.02] (20 elementi) & 0 \\
    \hline
    \end{tabular}
\end{center}
\textbf{Output}: dataframe preprocessato e rimescolato.
\begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{index} & \textbf{text} & \textbf{label} \\
    \hline
        3 & [-0.07, 0.31, ..., 0.02] (20 elementi) & 0 \\
        0 & [0.02, 0.03, ..., 0.04] (20 elementi) & 1 \\
        2 & [0.04, 0.01, ..., 0.91] (20 elementi) & 0 \\
        1 & [-0.01, -0.03, ..., 0.03] (20 elementi) & 1 \\
    \hline
    \end{tabular}
\end{center}

\paragraph{Standardizzazione}\label{standardization}
Un passaggio finale della pipeline riguarda un'eventuale standardizzazione dei dati. In letteratura sono presenti molteplici opzioni, ad esempio la trasformazione \textit{min-max} oppure quella basata sul \textit{range interquartile}. In questo caso è stata scelta la tecnica che mira a calcolare i punti \textit{z-score}: si applica ad ogni osservazione la seguente trasformazione:
\begin{equation}
    z_i = \frac{x_i - \mu}{\sigma},
\end{equation}
in cui $x_i$ indica l'$i$-esima osservazione e $\mu$ e $\sigma$, rispettivamente la media e la deviazione standard campionarie della distribuzione delle feature; si ottiene, quindi, una distribuzione standardizzata centrata nell'origine e con deviazione standard pari a 1.

\section{Selezione dei modelli}\label{modelselection}
Nel processo di apprendimento di un task predittivo è possibile generare numerosi modelli, ciascuno determinato da una specifica configurazione di iperparametri. L'utilizzo di un criterio, come la grid search, permette di individuare quei valori degli iperparametri che determinano la generazione dei modelli che ottengono le prestazioni migliori.

\subsection{Grid search}\label{gs}
La grid search è una tecnica di ricerca del modello migliore che, sostanzialmente, automatizza il processo di tuning degli iperparametri.

Il termine grid deriva dal fatto che per ogni iperparametro si considera un insieme di valori candidati e si forma la griglia relativa al prodotto cartesiano di tutti questi insiemi, definendo tutte le possibili configurazioni per il tuning.

Un esempio di griglia viene mostrato in Tabella~\ref{grid}: in questo caso è stato fissato il tipo di fuzzificatore, il tipo di kernel e il tipo di solver, mentre è stato previsto un intervallo di undici ordini di grandezza per $\sigma$, parametro del kernel gaussiano, e per $C$.
\begin{table}
\centering
 \begin{tabular}{|c|c|c|c|c|} 
 \hline
\textbf{fuzzificatore} & \textbf{kernel} & \textbf{solver} & $\bm{\sigma}$ & \textit{\textbf{C}} 
\\ [0.5ex] 
 \thickhline
 lineare & gaussiano & gurobi & logspace(-5, 5, 11) & logspace(-5, 5, 11) \\ 
 \hline
\end{tabular}
\caption{Esempio di griglia di iperparametri per la grid search relativa ai modelli generati dall'algoritmo $\mu$-learn. La funzione \textit{logspace} permette di specificare l'intervallo di esponenti in base 10: in questo caso vengono ottenuti dei valori da $10^{-5}$ a $10^5$.}.
\label{grid}
\end{table}
Chiaramente, più ampio è il set di iperparametri da ottimizzare, più computazionalmente costosa diventa l'esecuzione dell'algoritmo, in quanto causa un aumento della dimensione della griglia. In questo senso, l'esempio in Tabella~\ref{grid} mostra anche come si possono fissare i valori di alcuni iperparametri per favorire la ricerca di quelli ottimali per altri parametri, alleggerendo, dunque, il carico computazionale. 

\paragraph{K-fold cross validation}
Uno dei modi possibili per valutare in maniera robusta i vari modelli considerati nella fase di tuning degli iperparametri è quello di utilizzare la cross validation. 
Nella sua forma più basilare, lo stratagemma utilizzato da questa tecnica consiste nel dividere il dataset $S$ in $k$ partizioni $S_1, S_2, ..., S_k$, dette fold, tali che:
\begin{equation}
     S = \bigcup\limits_{i=1}^{k} S_{i},
\end{equation}
\begin{equation}
    S_i \cap S_j = \emptyset \ \ \forall i \neq j.
\end{equation}
Si denota con $S^{(i)} \equiv S \setminus S_i$ la parte per l'addestramento e $S_i$ la parte per la validazione, come illustrato in Figura~\ref{cv}.
Per completezza, nell'Algoritmo~\ref{kfold} vengono delineati i passi chiave della procedura: fissati gli iperparametri, si partiziona in $k$ fold\footnote{Il partizionamento tipicamente tiene conto della rappresentatività delle fold ottenute: in \textit{scikit-learn} esistono delle funzioni come \textit{StratifiedKFold} che permettono di operare una suddivisione tale che le fold abbiano un rapporto bilanciato delle occorrenze dei valori delle etichette. Naturalmente, questo approccio si basa su valori discreti e, dunque, si presta esclusivamente a problemi di classificazione. Nel caso della regressione è possibile applicare questa suddivisione solo discretizzando i valori delle etichette, ad esempio considerando per ciascuna l'intero più vicino.} e, per ogni iterazione, si genera un modello addestrato su $S^{(i)}$ e validato su $S_i$, con le relative predizioni valutate tramite le metriche definite nel Paragrafo~\ref{evaluation}. Infine, le misure di valutazione ottenute vengono aggregate tramite la media.

\begin{figure}
    \centering
    \includegraphics[scale=0.83]{images/cv.png}
    \caption{Schematizzazione del processo di cross validation: in rosso la parte per la validation e in verde la parte per il training.}
    \label{cv}
\end{figure}
\begin{algorithm}
\caption{\texttt{k-fold cross validation}}
\label{kfold}
\hspace*{\algorithmicindent} \textbf{Procedure} cv($S$, $k$)
\newline
\hspace*{\algorithmicindent} \textbf{Input}: $S$ è il dataset, $k$ il numero di fold
\newline
\hspace*{\algorithmicindent} \textbf{Output}: $k$ modelli e la media delle loro valutazioni
\begin{algorithmic}[1]
\STATE $S_1, S_2, ..., S_k$ = partitioning($S$, $k$)
\FOR{$S_i$ in $S_1, S_2, ..., S_k$}
\STATE validation = $S_i$
\STATE train = $S \setminus S_i$
\STATE mulearn.configure(hyperparameters)
\STATE model = mulearn.fit(train.x, train.y)
\STATE predictions = model.predict(validation.x)
\STATE evaluation = model.evaluate(predictions, validation.y)
\STATE models.add(model)
\STATE evaluations.add(evaluation)
\ENDFOR
\STATE cv\_evaluation = mean(evaluations)
\RETURN models, cv\_evaluation
\end{algorithmic}
\end{algorithm}
Al fine di valutare in modo robusto la bontà di generalizzazione del modello generato al termine della model selection, è stata implementata quella che viene definita \textit{cross validation annidata}: si specifica una griglia di valori per gli iperparametri, si opera una cross validation esterna su $k$ fold esterne e si partiziona la parte per il training in $l$ fold interne. Per ogni configurazione dei valori nella griglia degli iperparametri si ripete l'operazione di cross validation sulle fold interne e si aggregano i punteggi delle metriche ottenute sul validation set nelle varie iterazioni.
In questo modo ogni configurazione è associata a un valore di performance, calcolato su dati non utilizzati per addestrare i modelli, ed è così possibile scegliere il modello migliore.
La procedura appena descritta viene illustrata nell'Algoritmo~\ref{nested}.
\begin{algorithm}
\caption{\texttt{cross validation annidata}}
\label{nested}
\hspace*{\algorithmicindent} \textbf{Procedure} nested\_cv($S$, $k$, $l$, $grid$)
\newline
\hspace*{\algorithmicindent} \textbf{Input}: $S$ è il dataset, $k$ il numero di fold esterne, $l$ il numero di fold interne, $grid$ la griglia di valori
\newline
\hspace*{\algorithmicindent} \textbf{Output}: $k$ modelli ottimizzati e la relativa media delle misure di valutazione considerate
\begin{algorithmic}[1]
\STATE $S_1, S_2, ..., S_k$ = partitioning($S$, $k$)
\FOR{$S_i$ in $S_1, S_2, ..., S_k$}
\STATE test = $S_i$
\STATE train = $S \setminus S_i$
\STATE $D_1, D_2, ..., D_l$ = partitioning(train, $l$)
\FOR{\textbf{each} configuration in $grid$}
\FOR{$D_j$ in $D_1, D_2, ..., D_l$}
\STATE in\_validation = $D_j$
\STATE in\_train = train$\setminus D_j$

\STATE mulearn.configure(configuration)
\STATE in\_model = mulearn.fit(in\_train.x, in\_train.y)
\STATE in\_predictions = in\_model.predict(in\_validation.x)
\STATE in\_evaluation = in\_model.evaluate(in\_predictions, in\_validation.y)
\STATE in\_evaluations.add(in\_evaluation)
\STATE mean\_in\_evaluation = mean(in\_evaluations)
\ENDFOR
\STATE mean\_in\_evalutations.add(mean\_in\_evaluation)
\ENDFOR
\STATE best\_configuration = get\_best(mean\_in\_evaluations)
\STATE mulearn.configure(best\_configuration)
\STATE optimized\_model = optimized\_model.fit(train.x, train.y)
\STATE predictions = optimized\_model.predict(test.x)
\STATE evaluation = optimized\_model.evaluate(predictions, test.y)
\STATE evaluations.add(evaluation)
\STATE optimized\_models.add(optimized\_model)
\ENDFOR
\STATE nested\_cv\_evaluation = mean(evaluations)
\RETURN optimized\_models, nested\_cv\_evaluation
\end{algorithmic}
\end{algorithm}

\section{Visualizzazione dei dati}\label{datavisualizationimpl}
La parte di visualizzazione dei dati è fondamentale per rappresentare i risultati degli esperimenti e, possibilmente, trarre delle considerazioni utili per migliorare il sistema. \`E stata, quindi, predisposta l'analisi descrittiva dei risultati che comprende:
\begin{itemize}
    \item la distribuzione delle predizioni dei modelli,
    \item la distribuzione delle etichette attese,
    \item il confronto con una baseline,
    \item il riepilogo delle misure di errore o accuratezza.
\end{itemize}
Durante lo svolgimento di questa tesi sono stati ricavati diversi report che hanno consentito di individuare la direzione in cui far proseguire gli esperimenti.

Nel prossimo capitolo verranno presentati gli esperimenti realizzati e gli esiti che tale analisi descrittiva ha permesso di riassumere.

\chapter{Valutazione sperimentale}
\label{Capitolo 4}
\onehalfspacing
In quest'ultimo capitolo si illustra nel dettaglio la consistente parte di esperimenti che è stata condotta durante il lavoro documentato da questa tesi.

Nel Paragrafo~\ref{dataset} vengono presentati i dataset che sono stati presi in considerazione per la sperimentazione del sistema descritto nel Capitolo~\ref{Capitolo 3}. Successivamente, all'interno del Paragrafo~\ref{esperimenti} vengono mostrati i risultati finali che sono stati prodotti.

\section{Dataset}\label{dataset}
All'interno del Paragrafo~\ref{datasethandle} sono stati definiti i possibili scenari per i dati tramite i quali il sistema di apprendimento è stato allenato. 
Dal momento che in letteratura il riconoscimento delle fake news è stato frequentemente modellato come un problema di classificazione, i dataset a disposizione sul Web in ambito di apprendimento supervisionato e inerenti alle notizie sono tipicamente etichettati con valori binari.
Per questa ragione il primo tipo di dataset considerato durante il lavoro di questa tesi sono dei dataset generati che hanno l'obiettivo di ottenere per le etichette dei valori compresi nell'intervallo $[0,1]$.

\subsection{Dataset generati}\label{generated_datasets}
Un approccio proposto per gli esperimenti è quello di utilizzare dei dataset generati tramite la tecnica descritta nel Paragrafo~\ref{generator}. Tale tecnica si basa sulla scelta di una distribuzione arbitraria $d$ che determina l'andamento dei valori del grado di appartenenza in funzione delle estrazioni di una variabile casuale. Al fine di considerare un ampio spettro di scenari sono state scelte due distribuzioni: la distribuzione uniforme e la distribuzione Beta, e da esse sono stati estratti dei campioni.

\paragraph{Distribuzione uniforme}
Nella teoria delle Probabilità una delle distribuzioni più conosciute è sicuramente quella uniforme continua. A partire da questa distribuzione è stato estratto il campione mostrato in Figura~\ref{uniforme}. Genericamente definita su un intervallo $[a,b]$, la distribuzione uniforme ha la seguente funzione densità di probabilità
\begin{equation}
    f(x)= \begin{cases} \frac{1}{b-a} & \mbox{se } a\leq x \leq b, \\ 0 & \mbox{altrimenti.} \end{cases}
\end{equation}
Dal momento che il grado di appartenenza è compreso nell'intervallo $[0,1]$, anche la distribuzione uniforme selezionata è impostata su tale intervallo. 
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/uniform.png}
    \caption{Campione estratto da una distribuzione uniforme.}
    \label{uniforme}
\end{figure}
\paragraph{Distribuzione Beta}
L'altro tipo di distribuzione scelta è la distribuzione Beta: a differenza di quella uniforme essa è specificatamente definita nell'intervallo $[0,1]$ e, dunque, naturalmente adatta al contesto di generazione del grado di appartenenza. I parametri che ne regolano l'andamento sono due: $\alpha$ e $\beta$. La funzione densità di probabilità è
\begin{equation}
    f(x)= \frac{x^{\alpha - 1}(1-x)^{\beta - 1}}{B(\alpha, \beta)},
\end{equation}
dove $B$ è la funzione
\begin{equation}
    B(\alpha, \beta) = \int_{0}^{1} x^{\alpha - 1}(1-x)^{\beta - 1} \,dx.
\end{equation}
A partire da questa distribuzione sono stati estratti due campioni a partire dai seguenti casi:
\begin{enumerate}
    \item $\alpha = \beta = 5$ (Figura~\ref{beta5}),
    \item $\alpha = \beta = 0.5$ (Figura~\ref{beta05}).
\end{enumerate}
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/beta5.png}
    \caption{Campione estratto da una distribuzione Beta con parametri $\alpha = \beta = 5$.}
    \label{beta5}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/beta05.png}
    \caption{Esempio di distribuzione approssimativamente Beta, con parametri $\alpha = \beta = 0.5$.}
    \label{beta05}
\end{figure}
Sono stati ottenuti, quindi, tre scenari: uno tramite l'estrazione di un campione a partire dalla distribuzione uniforme e due estratti dai due casi per la distribuzione Beta riportati sopra. Tali scenari consentono di testare la capacità dell'algoritmo di indurre la funzione di appartenenza in tre situazioni molto differenti. 
Il primo caso coinvolge gradi di appartenenza che hanno equa probabilità di occorrere, il secondo ha le sembianze di una distribuzione normale troncata nell'intervallo $[0,1]$; in questo caso vengono generati gradi di appartenenza che tendono a concentrarsi attorno al valore 0.5 e quindi lo scenario è quello di un dataset con una forte incertezza. Infine, il terzo scenario riproduce un fenomeno di polarizzazione delle etichette ai valori estremi dell'intervallo.

\subsubsection{Insiemi di parole} Il generatore di dataset, tramite le distribuzioni appena descritte, è in grado di creare dei documenti testuali sintetici a partire da insiemi di parole rappresentative di due topic.
Da un punto di vista lessicale, il problema non è banale e andrebbe approfondito determinando con cura l'insieme delle parole che contraddistinguono una notizia vera piuttosto che una fake.
Per questa parte di esperimenti sono state poste le basi per fare delle prove da una prospettiva puramente sintattica, con l'intenzione di fornire un punto di partenza per esperimenti futuri più accurati. Si definiscono, quindi, gli insiemi
\begin{center}
    $P_1$ = \texttt{\{arrogant, uptight, toothpaste, bottle, grass, show, ..., religion\}} (156 elementi)
\end{center}
\begin{center}
    $P_2$ = \texttt{\{ARROGANT, UPTIGHT, TOOTHPASTE, BOTTLE, GRASS, SHOW, ..., RELIGION\}} (156 elementi)
\end{center}
composti da termini inglesi randomici la cui semantica non viene considerata per le rilevazione delle fake news. Inoltre, il fatto di avere parole in maiuscolo o minuscolo aiuta a valutare visualmente l'appartenenza dei documenti prodotti, in senso fuzzy, alle due categorie di news. Senza perdita di generalità si considera $P_1$ l'insieme rappresentativo del topic delle notizie vere e $P_2$ l'insieme associato alle fake news.

\paragraph{Insiemi disgiunti}
Dal momento che $P_1$ contiene solo parole scritte in caratteri minuscoli e $P_2$ parole in maiuscolo, allora si ha che $P_1$ $\cap$ $P_2 = \emptyset$. Si parla, quindi, di insiemi disgiunti e la generazione di dataset avviene a partire da questi due insiemi. In Tabella~\ref{generationexample} viene riportato un esempio di generazione di documenti testuali al variare del grado di appartenenza. In questo caso è stato fissato come successo l'evento che si verifica quando la parola generata appartiene all'insieme $P_2$; di conseguenza, all'aumentare di $\mu_i$ si osserva una crescente componente di parole maiuscole all'interno del documento generato.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{id} & \textbf{esempio di testo ottenuto (1000 parole)} & $\bm{\mu_i}$  
\\ [0.5ex] 
 \thickhline
0 & ``extend bottle bottle extend toothpaste SHOW grass cuddly uptight uptight eager ... violet" & 0.07 \\ 
1 & ``uptight bottle toothpaste occur scream SUDDEN show toothpaste provide violet embarrass ... wistful" & 0.14 \\
2 & ``thick QUILL toothpaste toothpaste BOTTLE BOTTLE uptight lyrical introduce SUPPORT toothpaste ... trees" & 0.32 \\
3 & ``BOTTLE TOOTHPASTE BOTTLE SMILING lyrical LIE VIOLET REMOVE YELLOW bottle BOTTLE ... MICE" & 0.49 \\
4 & ``DREARY NEAR UPTIGHT uptight MICE bottle bottle trousers uptight UPTIGHT UPTIGHT ... mice" & 0.55\\
5 & ``USE violet MICE EXTEND show INTRODUCE DESCRIBE show uptight DRAG UPTIGHT ... DELICIOUS" & 0.57\\
6 & ``BOTTLE UPTIGHT YUMMY show GRASS HOLIDAY rare SHOW EAGER UPTIGHT hulking ... UPTIGHT" & 0.64\\
7 & ``uptight OUTSTANDING tedious stem SHOW DUCKS BOTTLE INTRODUCE OBSOLETE HOMELESS FIELD ... SHOW" & 0.69 \\
8 & ``toothpaste fly UPTIGHT BARBAROUS RIGHT BOTTLE SHOW uptight TOOTHPASTE BOTTLE uptight ... ADORABLE" & 0.75\\
9 & ``UPTIGHT oval APATHETIC COLLECT racial TOOTHPASTE TOOTHPASTE UPTIGHT GRASS QUILL BOTTLE ... MICE" & 0.83\\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Esempio di testi sintetici ottenuti a partire da $P_1 \cap P_2 = \emptyset$, al variare di $\mu_i$.}
\label{generationexample}
\end{table}

\paragraph{Insiemi sovrapposti}
Realisticamente parlando, l'insieme delle parole occorrenti nelle fake news non è disgiunto da quello delle parole presenti nelle notizie vere.
Per questa ragione sono stati modificati gli insiemi di partenza in modo che ciascuno dei due contenesse una frazione del $17 \%$ di elementi appartenenti all'altro insieme. In questa maniera sono stati ottenuti gli insiemi
\begin{center}
    $P_{1}^{*}$ = \texttt{\{arrogant, UPTIGHT, toothpaste, bottle, grass, SHOW, ..., religion\}} (156 elementi)
\end{center}
\begin{center}
    $P_{2}^{*}$ = \texttt{\{ARROGANT, UPTIGHT, toothpaste, BOTTLE, GRASS, SHOW, ..., religion\}} (156 elementi)
\end{center}
di conseguenza, $P_{1}^{*}$ $\cap$ $P_{2}^{*} \neq \emptyset$.

Il processo di generazione è stato ripetuto e sono stati ottenuti nuovi documenti, come mostrato in Tabella~\ref{generationexample2}.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{id} & \textbf{esempio di testo ottenuto (1000 parole)} & $\bm{\mu_i}$ 
\\ [0.5ex] 
 \thickhline
0 & ``occur stale eager UPTIGHT grass lyrical grass UPTIGHT PARTNER UPTIGHT UPTIGHT ... toothpaste" & 0.02 \\
1 & ``stem drag HALL SHOW ticket bottle bottle SHOW UPTIGHT stimulating mice ... GRASS" & 0.14  \\
2 & ``UPTIGHT lyrical UPTIGHT lyrical toothpaste grass grass UPTIGHT homeless UPTIGHT BOTTLE ... drag" & 0.24 \\
3 & ``hall hall USE sleet bubble toothpaste DELICIOUS RIGHT MERE UPTIGHT UPTIGHT ... delicious" & 0.32 \\
4 & ``smiling HOLIDAY absent YELLOW SHOW BUBBLE UPTIGHT UPTIGHT minor DELICIOUS toothpaste ... UPTIGHT" & 0.45 \\
5 & ``PARTNER QUILL SOPHISTICATED UPTIGHT SHOW BAG DESCRIBE ticket UPTIGHT UPTIGHT grass ... sudden" & 0.55 \\
6 & ``SHOW UPTIGHT GATE UPTIGHT BOTTLE GRASS FLOAT UPTIGHT TROUSERS QUILL LYRICAL ... WRONG" & 0.60  \\
7 & ``LIMPING toothpaste toothpaste TENT delicious QUILL SHOW VIOLET LYRICAL SUDDEN SHOW ... UPTIGHT" & 0.75 \\
8 & ``UPTIGHT SOPHISTICATED SHOW TENT LYRICAL UPTIGHT peace lyrical MICE EARTH WELL-GROOMED ... UPTIGHT" & 0.84 \\
9 & ``GRASS PEACE BOTTLE BOTTLE UPTIGHT MICE DREARY UPTIGHT PSYCHOTIC BUBBLE REMOVE ... EMBARRASS" & 0.94 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Esempio di testi sintetici a partire da $P_{1}^{*} \cap P_{2}^{*} \neq \emptyset$, al variare di $\mu_i$.}
\label{generationexample2}
\end{table}
In generale, una volta generato un dataset in uno dei due modi visti in precedenza, esso viene elaborato dal modulo di preprocessing,
ottenendo una rappresentazione simile a quella mostrata nella Tabella~\ref{uniformlda}.
Nell'esempio appena indicato si mostra lo specifico caso dell'output della pipeline a partire da un dataset generato tramite la tecnica LDA, tuttavia occorre tenere presente che lo stesso procedimento vale per i dataset che sono stati raccolti da terzi e che contengono le notizie. Come si evince dalla didascalia della Tabella~\ref{uniformlda}, la scelta dei passi della pipeline di preprocessing possono variare, a seconda del tipo di dataset a disposizione.

\begin{table}
\centering
 \begin{tabular}{|c|c|c|} 
 \hline 
 \textbf{index} & \textbf{text} & \textbf{membership}
\\ [0.5ex] 
 \thickhline
67 & [0.55, -0.34, ..., -1.63] (96 elementi) & 0.59 \\
31 & [0.90, -0.40, ..., -1.61] (96 elementi) & 0.83\\ 
2 & [1.07, -0.48, ..., -1.65] (96 elementi) & 0.97\\
99 & [0.31, -0.21, ..., -1.63] (96 elementi) & 0.48\\
17 & [0.91, -0.38, ..., -1.65] (96 elementi) & 0.80\\
29 & [0.92, -0.42, ..., -1.67] (96 elementi) & 0.83\\
 \hline
\end{tabular}
\caption{Campione del dataset generato tramite LDA basata su distribuzione uniforme e da insiemi disgiunti, una volta che è stato preprocessato. La pipeline di preprocessing scelta in questo caso conteneva soltanto i passi di embedding (word2vec) e la conseguente fase di aggregazione dei valori.}
\label{uniformlda}
\end{table}

\subsection{Dataset raccolti da terzi}
Durante il lavoro di questa tesi, oltre ai dataset generati, sono state prese in considerazione delle raccolte di dati con etichette binarie. Normalmente, questo tipo di dati è destinato al task della classificazione; tuttavia, per la parte di esperimenti, dati di questo tipo sono stati utilizzati anche per l'induzione del grado di appartenenza all'insieme fuzzy delle fake news. In generale, dopo una prima fase di ricerca sul Web, sono state individuate le seguenti possibilità:
\begin{enumerate}
    \item \texttt{Fake and real news\footnote{https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset}} - 44898 osservazioni - 111 MB,
    \item \texttt{All the News 2.0\footnote{https://components.one/datasets/all-the-news-2-news-articles-dataset/}} - 2.7 milioni di osservazioni - 9.2 GB,
    \item \texttt{ProfNER\footnote{https://temu.bsc.es/smm4h-spanish/}} - 10000 osservazioni - 13.6 MB.


\end{enumerate}
Il primo dataset comprende sia delle notizie pubblicate su testate giornalistiche negli Stati Uniti sia dei tweet, il secondo presenta un'ampia raccolta di notizie e di saggi su 27 diverse pubblicazioni americane mentre il terzo contiene dei tweet in lingua spagnola.

Al fine di trovare un compromesso accettabile tra il numero di esperimenti e la capacità computazionale a disposizione è stato scelto il primo dataset.

\subsubsection{\textit{Fake and real news}}\label{fakenewsdataset}
\textit{Fake and real news} è un dataset, disponibile sulla piattaforma \textit{Kaggle}, che raccoglie notizie e tweet, separandoli in ``fake'' e ``true'' news. Più precisamente sono presenti 23481 osservazioni per la parte di notizie false e 21417 per quelle annotate come vere, formando, quindi, un dataset approssimativamente bilanciato. 
Le osservazioni sono composte da informazioni inerenti al titolo della notizia, al corpo, all'argomento in essa contenuto e alla data di pubblicazione. Nelle Figure~\ref{fake_sample} e~\ref{true_sample} viene mostrato un campione del dataset per ciascuna categoria di notizie.
\begin{figure}
    \centering
    \includegraphics[scale=0.52]{images/fake_sample.png}
    \caption{Campione delle notizie del dataset etichettate come ``fake''.}
    \label{fake_sample}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/true_sample.png}
    \caption{Campione delle notizie del dataset etichettate come ``vere''.}
    \label{true_sample}
\end{figure}
Si osserva la presenza di un'informazione temporale relativa al giorno di pubblicazione della notizia: questo rimanda alle premesse descritte nel Paragrafo~\ref{pp} che forniscono una motivazione convincente del perché, in casi come questi, la fase di shuffling sia necessaria al fine di prevenire un sovradattamento dovuto a un tale ordinamento.

La colonna relativa al titolo, per quanto rilevante, non è stata considerata ai fini dell'addestramento. La motivazione riguarda principalmente la presenza di tweet: in quel caso è difficile, infatti, parlare di ``titolo'', ed è tendenzialmente anche poco informativo (lo si evince facilmente dalla riga 8 del campione mostrato in Figura~\ref{true_sample}). Si presume, quindi, che tale informazione possa essere usata in congiunzione con altre come supporto.
Anche la colonna relativa all'argomento non è stata utilizzata, in quanto è stata rilevata una sostanziale differenza nella ricchezza di informazioni contenute per la parte di notizie vere rispetto a quelle fake. Inoltre, a true e fake news risultano associati insiemi di argomenti disgiunti. Si conferma questo aspetto nella Tabella~\ref{subject}.
\begin{table}
\centering
 \begin{tabular}{|c|c|c|} 
 \hline 
 \textbf{argomento} & \textbf{fake news} & \textbf{true news}
\\ [0.5ex] 
 \thickhline
\texttt{News} & 38.54 \% & - \\
\texttt{politics} & 29.13 \% & -\\ 
\texttt{left-news} & 18.99 \% & -\\
\texttt{Government News} & 6.69 \% & -\\
\texttt{US\_News} & 3.33 \% & -\\
\texttt{Middle-east} & 3.31 \% & -\\
\texttt{politicsNews} & - & 52.63 \% \\
\texttt{worldnews} & - & 47.37 \% \\
 \hline
\end{tabular}
\caption{Frequenze relative degli argomenti.}
\label{subject}
\end{table}
Per questa parte di esperimenti si è deciso, quindi, di utilizzare unicamente l'informazione riguardante il corpo delle notizie. 
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/pipeline_inst.png}
    \caption{Pipeline di preprocessing utilizzata per i due tipi di notizie.}
    \label{pipe}
\end{figure}
Dal momento che le notizie sono separate in due dataframe differenti, si è scelto di utilizzare due istanze della pipeline di preprocessing mostrata in Figura~\ref{pipe}: una per le fake news e una dedicata alle true news. In questo modo sono stati ottenuti dei vettori di feature a cui sono stati associati i valori binari delle etichette e, infine, sono stati accorpati, rimescolati e standardizzati, formando un unico dataset: un esempio di risultato è quello mostrato in Tabella~\ref{preprocesseddata}.
\begin{table}
\centering
 \begin{tabular}{|c|c|c|} 
 \hline 
 \textbf{index} & \textbf{text} & \textbf{label}
\\ [0.5ex] 
 \thickhline
22 & [0.34, 0.14, ..., 0.51] (96 elementi) & 1 \\
6 & [-0.45, -0.10, ..., 0.55] (96 elementi) & 0 \\ 
59 & [0.91, -0.92, ..., -1.14] (96 elementi) & 0\\
21 & [-0.86, 0.54, ..., 0.51] (96 elementi) & 1\\
12 & [0.11, 0.67, ..., -1.95] (96 elementi) & 0\\
132 & [0.83, -0.37, ..., 0.85] (96 elementi) & 1\\
 \hline
\end{tabular}
\caption{Campione del dataset \textit{Fake and real news}, una volta preprocessato.}
\label{preprocesseddata}
\end{table}

\subsubsection{Altri potenziali dataset}
Gli altri due dataset citati nel Paragrafo~\ref{dataset} non sono ancora stati considerati per l'addestramento e per la conseguente generazione di modelli predittivi; tuttavia, si ritiene che il sistema sia effettivamente pronto per una loro futura elaborazione, in quanto presenta un modulo per l'adattamento del dataset (Paragrafo~\ref{adapter}) e, in generale, è stato predisposto per trattare documenti testuali in lingua italiana, inglese, francese e spagnola.

\section{Esperimenti}\label{esperimenti}
Durante il lavoro di questa tesi sono stati condotti circa 30 esperimenti che hanno permesso di valutare un'ampia gamma di scenari. Si ritiene che gli esperimenti riportati in questo paragrafo siano i più rappresentativi dell'intero processo di test del sistema creato. Ciascuno di essi viene presentato in due varianti, mostrando il comportamento del predittore $\lambda$ e del classificatore $\omega$ (Paragrafo~\ref{predictors}); pertanto, si utilizzerà per il primo la misura di RMSE e per il secondo le metriche di Precision, Recall e F1, distinguendo in training e test error. 

Quella che segue è la lista degli esperimenti che sono stati pianificati.

\paragraph{Prima parte - documenti testuali generati:}
\begin{itemize}
    \item \texttt{E1} - dataset generato da insiemi \textit{disgiunti} a partire da una distribuzione \textit{uniforme} nell'intervallo $[0,1]$,
    \item \texttt{E2} - dataset generato da insiemi \textit{disgiunti} a partire da una distribuzione \textit{Beta} con $\alpha=\beta=5$,
    \item \texttt{E3} - dataset generato da insiemi \textit{disgiunti} a partire da una distribuzione \textit{Beta} con $\alpha=\beta=0.5$,
    \item \texttt{E4} - dataset generato da insiemi \textit{sovrapposti} a partire da una distribuzione \textit{uniforme} nell'intervallo $[0,1]$,
    \item \texttt{E5} - dataset generato da insiemi \textit{sovrapposti} a partire da una distribuzione \textit{Beta} con $\alpha=\beta=5$,
    \item \texttt{E6} - dataset generato da insiemi \textit{sovrapposti} a partire da una distribuzione \textit{Beta} con $\alpha=\beta=0.5$.
\end{itemize}
\paragraph{Seconda parte - notizie:}
\begin{itemize}
    \item \texttt{E7} - \textit{Fake and real news dataset} (Paragrafo~\ref{fakenewsdataset}).
\end{itemize}
La valutazione sperimentale prevede anche un paragone tra i predittori proposti e alcuni di quelli già presenti in letteratura; per questa ragione, è stato predisposto un confronto con gli algoritmi Random Forest per la regressione e per la classificazione contestualmente alla prima parte di esperimenti sui dati generati. Nella seconda parte relativa al dataset presente su Kaggle è stata prevista una baseline composta da un albero di decisione, una rete neurale multi-livello, un classificatore suppor vector e due algoritmi Random Forest per la regressione e per la classificazione.

In Tabella~\ref{gridtable} viene mostrata la griglia comune che è stata utilizzata per la ricerca degli iperparametri ottimali per tutti gli esperimenti che riguardano $\mu$-learn; inoltre, la cross validation k-fold annidata utilizzata è stata impostata per eseguire 5 fold esterne e 4 interne: a conferma di ciò, ogni esperimento documentato riporta la selezione di 5 modelli ottimali, indicando la media aritmetica degli errori di train e di test ottenuti nelle varie fold.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|c|c|} 
 \hline
\textbf{fuzzificatore} & \textbf{kernel} & \textbf{solver} & $\bm{\sigma}$ & \textit{\textbf{C}} 
\\ [0.5ex] 
 \thickhline
 lineare & gaussiano & gurobi & $(10^{-5}, 10^{-3}, 10^{-1}, 10, 10^3, 10^5)$ & (0.01, 0.1, 0.5, 1.0, 10, 100) \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Griglia di parametri per l'algoritmo $\mu$-learn.}
\label{gridtable}
\end{table}
Per quanto riguarda la baseline, invece, in Tabella~\ref{gridtablebaseline} viene mostrata la griglia di valori relativa alla model selection degli algoritmi Random Forest, mentre nel caso dell'albero di decisione, della rete neurale e del classificatore support vector non è stata fatta un'ulteriore selezione dei modelli a causa del limitato tempo a disposizione durante il lavoro documentato da questa tesi.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|c|c|c|} 
 \hline
\textbf{bootstrap} & \textbf{max profondità} & \textbf{max feature} & \textbf{min foglie campione} & \textbf{min partizioni campione} & \textbf{numero stimatori}
\\ [0.5ex] 
 \thickhline
 Sì & (80, 90, 100, 110) & (2, 3) & (3, 4, 5) & (8, 10, 12) & (100, 200, 300, 1000) \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Griglia di parametri per i due algoritmi di Random Forest considerati.}
\label{gridtablebaseline}
\end{table}
Infine, si specifica che, per consentire un buon numero di esperimenti, essi sono stati effettuati su un dataset di 1000 osservazioni\footnote{Si ritiene che questo sia un buon compromesso tra il tempo di esecuzione di un singolo esperimento di apprendimento e l'utilizzo di dataset sufficientemente grandi da essere informativi.} producendo tale numero di documenti testuali nel caso dei dataset generati ed estraendo casualmente un campione, invece, nel caso del dataset \textit{Fake and real news}.

\subsection{Esperimenti con i dataset generati}
In questo paragrafo si illustra la prima parte degli esperimenti effettuati sui dataset generati nel modo descritto nel Paragrafo~\ref{generated_datasets}. Come specificato nel Paragrafo~\ref{esperimenti}, sono stati creati sei casi di dataset sintetici tramite il generatore di dataset descritto nel Paragrafo~\ref{generator}. Successivamente, tramite il resto del sistema descritto nel Paragrafo~\ref{sistema}, sono stati selezionati i modelli generati dall'algoritmo $\mu$-learn e visualizzati i risultati.
La documentazione degli esperimenti si compone in due livelli di analisi: il primo suddivide gli insiemi di partenza in insiemi disgiunti e parzialmente sovrapposti; il secondo livello differenzia ciascun esperimento per il tipo di distribuzione scelta per la generazione dei dati. In particolare, è stata utilizzata una distribuzione uniforme nell'intervallo $[0,1]$, una distribuzione Beta con parametri $\alpha = \beta = 5$ e una con $\alpha = \beta = 0.5$. Ciò che caratterizza questi esperimenti è la presenza di etichette nella forma di valori continui compresi nell'intervallo $[0,1]$.

\subsubsection{Insiemi disgiunti}\label{insiemidisgiunti}
Gli esperimenti relativi agli insiemi disgiunti sono basati su insiemi di parole di partenza che non si sovrappongono. Seguono i tre casi di distribuzione sopracitati.

\paragraph{E1 - Distribuzione uniforme} Il primo esperimento considera un dataset generato da insiemi di parole disgiunti tramite una distribuzione uniforme continua nell'intervallo $[0,1]$. I modelli selezionati vengono indicati nella Tabella~\ref{models_exp1}.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|} 
 \hline $\bm{\sigma}$ & \textit{\textbf{C}}
\\ [0.5ex] 
 \thickhline
 1000 & 1 \\
 10 & 0.01 \\
 1000 & 1 \\
 10 & 0.01 \\
 1000 & 10 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da $\mu$-learn nell'esperimento \texttt{E1}.}
\label{models_exp1}
\end{table}
\paragraph{E1 - Confronto con $\bm{\lambda}$}
In Figura~\ref{prediction_exp1} viene riportato un confronto tra la ground truth dei dati generati e i gradi di appartenenza predetti dai modelli con il predittore $\lambda$. In particolare, si osserva come l'andamento della predizioni non sia propriamente fedele alle etichette attese e come, inoltre, sia evidente nella predizioni un picco di valori nella fascia $[0.5, 0.75]$.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/prediction_memberships.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le predizioni effettuate da $\lambda$ nell'esperimento \texttt{E1}.}
    \label{prediction_exp1}
\end{figure} 

Al fine di paragonare il comportamento dei modelli generati dall'algoritmo $\mu$-learn, sono state calcolate le predizioni da parte dell'algoritmo Random Forest (Figura~\ref{rf_exp1}). Anche in questo caso, tali predizioni sono state generate a partire dai migliori modelli selezionati tramite la grid search; i valori ottimali degli iperparametri dei modelli sono riportati in Tabella~\ref{models_rf_exp1}.

\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|c|c|} 
 \hline
\textbf{max profondità} & \textbf{max feature} & \textbf{min foglie campione} & \textbf{min partizioni campione} & \textbf{numero stimatori}
\\ [0.5ex] 
 \thickhline
 80 & 3 & 4 & 8 & 1000 \\ 
 80 & 3 & 4 & 8 & 100 \\ 
 80 & 3 & 4 & 8 & 1000 \\ 
 80 & 3 & 3 & 8 & 1000 \\ 
 80 & 3 & 3 & 8 & 1000 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da Random Forest nell'esperimento \texttt{E1}.}
\label{models_rf_exp1}
\end{table}

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/prediction_regression_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la regressione effettuata dall'algoritmo Random Forest nell'esperimento \texttt{E1}.}
    \label{rf_exp1}
\end{figure} 
Dai grafici si evince chiaramente una notevole capacità da parte di Random Forest nel replicare fedelmente la distribuzione impostata per la generazione.
Per questa serie di esperimenti relativa ai dati generati sono stati calcolati i punteggi medi di RMSE per il training e il test error (Tabella~\ref{rmse_exp1}). 
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \textbf{Training error} & \textbf{Test error}
\\ [0.5ex] 
 \thickhline
 $\lambda$ & 0.18 & 0.18 \\
 regressore random forest & \textbf{0.01} & \textbf{0.01}
 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{valori medi di RMSE per il predittore $\lambda$ e Random Forest nell'esperimento \texttt{E1}.}
\label{rmse_exp1}
\end{table}
I numeri confermano la superiorità da parte del regressore nell'individuare il grado di appartenenza originale dei documenti.

\paragraph{E1 - Confronto con $\bm{\omega}$}
Per quanto concerne la classificazione, in Figura~\ref{classification_exp1} viene fatta una valutazione del classificatore $\omega$. 
Più precisamente, in Figura~\ref{4cases_exp1} vengono mostrati i grafici relativi alle quattro situazioni descritte nella matrice di confusione. Quest'ultima è stata ricavata tramite il comportamento di $\omega$ che effettua la classificazione tramite una funzione di soglia di 0.5 sulle predizioni di $\lambda$: per ognuna delle quattro categorie il grafico riporta l'istogramma dei gradi di appartenenza predetti da $\lambda$, permettendo di vedere in quali casi essi determinano una classificazione corretta oppure errata da parte di $\omega$.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/prediction_classification.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le classificazioni effettuate da $\omega$ nell'esperimento \texttt{E1}.}
    \label{classification_exp1}
\end{figure}
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/tn.png}\label{tn_ud}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/tp.png}\label{tp_ud}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/fn.png}\label{fn_ud}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/fp.png}\label{fp_ud}
   \end{minipage}
   \caption{Gradi di appartenenza predetti da $\lambda$ e usati da $\omega$ per effettuare la classificazione. In alto a sinistra i veri negativi, a destra i veri positivi; in basso a sinistra i falsi negativi e a destra i falsi positivi nell'esperimento \texttt{E1}.}
   \label{4cases_exp1}
\end{figure}
 In Figura~\ref{rf_class_exp1}, invece, viene riportato il confronto tra le etichette attese e il classificatore Random Forest.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_disgiunti/prediction_classification_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la classificazione effettuata da Random Forest nell'esperimento \texttt{E1}.}
    \label{rf_class_exp1}
\end{figure} 
Anche in questo caso, l'algoritmo Random Forest si rivela particolarmente efficace. Si conferma questo aspetto nella Tabella~\ref{prf_exp1}, in cui vengono riportati i punteggi medi di Precision, Recall e F1.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \begin{tabular}{@{}c@{}}\textbf{Training} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular} & \begin{tabular}{@{}c@{}}\textbf{Test} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular}
\\ [0.5ex] 
 \thickhline
 $\omega$ & 0.75, 0.97, 0.85 & 0.77, 0.97, 0.86 \\
 classificatore random forest & \textbf{0.99, 1.0, 1.0} & \textbf{0.99, 0.99, 0.99} \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Valori medi di Precision, Recall e F1 per il predittore $\omega$ e la baseline nell'esperimento \texttt{E1}.}
\label{prf_exp1}
\end{table}

\paragraph{E2 - Distribuzione Beta con $\bm{\alpha=\beta=5}$}
Nel secondo esperimento si considera i documenti testuali generati da una distribuzione Beta con parametri $\alpha=\beta=5$ con gli insiemi di parole disgiunti; i modelli selezionati tramite la grid search vengono riportati nella Tabella~\ref{models_exp2}.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|} 
 \hline
$\bm{\sigma}$ & \textit{\textbf{C}}
\\ [0.5ex] 
 \thickhline
 10 & 0.01 \\
 100000 & 10 \\
 1000 & 1 \\
 10 & 0.01 \\
 10 & 0.01 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da $\mu$-learn nell'esperimento \texttt{E2}.}
\label{models_exp2}
\end{table}
\paragraph{E2 - Confronto con $\bm{\lambda}$}
Analogamente a quanto fatto nell'esperimento \texttt{E1}, si riporta in Figura~\ref{prediction_exp2} il confronto tra la ground truth dei dati generati e i gradi di appartenenza indotti dai dati con il predittore $\lambda$.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/prediction_memberships.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le predizioni effettuate da $\lambda$ nell'esperimento \texttt{E2}.}
    \label{prediction_exp2}
\end{figure} 

Anche in questo caso vengono raffigurate le predizioni dell'algoritmo Random Forest (Figura~\ref{rf_exp2}), per cui è stata eseguita la grid search per la ricerca dei parametri ottimali; i valori di tali parametri sono presenti in Tabella~\ref{models_rf_exp2}.

\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|c|c|c|} 
 \hline
\textbf{max profondità} & \textbf{max feature} & \textbf{min foglie campione} & \textbf{min partizioni campione} & \textbf{numero stimatori}
\\ [0.5ex] 
 \thickhline
 80 & 3 & 3 & 10 & 200 \\ 
 80 & 3 & 3 & 8 & 1000 \\ 
 80 & 3 & 3 & 10 & 200 \\ 
 80 & 3 & 3 & 12 & 300 \\ 
 80 & 3 & 3 & 8 & 1000 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da Random Forest nell'esperimento \texttt{E2}.}
\label{models_rf_exp2}
\end{table}

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/prediction_regression_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la regressione effettuata dall'algoritmo Random Forest nell'esperimento \texttt{E2}.}
    \label{rf_exp2}
\end{figure} 

Sono stati, dunque, calcolati i punteggi medi di RMSE per il training e il test error (Tabella~\ref{rmse_exp2}). 
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \textbf{Training error} & \textbf{Test error}
\\ [0.5ex] 
 \thickhline
 $\lambda$ & 0.14 & 0.14 \\
 regressore random forest & \textbf{0.01} & \textbf{0.02}
 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{valori medi di RMSE per il predittore $\lambda$ e Random Forest nell'esperimento \texttt{E2} nell'esperimento \texttt{E2}.}
\label{rmse_exp2}
\end{table}

\paragraph{E2 - Confronto con $\bm{\omega}$}
Nel caso della classificazione, la Figura~\ref{classification_exp2} illustra il confronto tra le etichette attese e le previsioni di $\omega$. 
In Figura~\ref{4cases_exp2} vengono mostrati i grafici inerenti alle quattro categorie presenti nella matrice di confusione.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/prediction_classification.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le classificazioni effettuate da $\omega$ nell'esperimento \texttt{E2}.}
    \label{classification_exp2}
\end{figure}
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/tn.png}\label{tn_b5d}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/tp.png}\label{tp_b5d}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/fn.png}\label{fn_b5d}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/fp.png}\label{fp_b5d}
   \end{minipage}
   \caption{Gradi di appartenenza predetti da $\lambda$ e usati da $\omega$ per effettuare la classificazione. In alto a sinistra i veri negativi, a destra i veri positivi; in basso a sinistra i falsi negativi e a destra i falsi positivi nell'esperimento \texttt{E2}.}
   \label{4cases_exp2}
\end{figure}
In Figura~\ref{rf_class_exp2} viene riportato il confronto tra le etichette attese e le predizioni del classificatore Random Forest in arancione.

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_disgiunti/prediction_classification_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la classificazione effettuata da Random Forest nell'esperimento \texttt{E2}.}
    \label{rf_class_exp2}
\end{figure} 
In Tabella~\ref{prf_exp2} vengono riportati i punteggi medi di Precision, Recall e F1.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \begin{tabular}{@{}c@{}}\textbf{Training} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular} & \begin{tabular}{@{}c@{}}\textbf{Test} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular}
\\ [0.5ex] 
 \thickhline
 $\omega$ & 0.62, 0.77, 0.69 & 0.63, 0.77, 0.69 \\
 classificatore random forest & \textbf{0.99, 0.98, 0.98} & \textbf{0.98, 0.96, 0.97} \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Valori medi di Precision, Recall e F1 per il predittore $\omega$ e la baseline nell'esperimento \texttt{E2}.}
\label{prf_exp2}
\end{table}
Da un punto di vista dell'efficacia dei due modelli, si confermano le considerazioni fatte per l'esperimento \texttt{E1}.

\paragraph{E3 - Distribuzione Beta con $\bm{\alpha=\beta=0.5}$}
Il terzo esperimento considera il dataset generato da insiemi disgiunti con una distribuzione Beta di parametri $\alpha=\beta=0.5$; i modelli selezionati sono mostrati nella Tabella~\ref{models_exp3}.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|} 
 \hline
$\bm{\sigma}$ & \textit{\textbf{C}}
\\ [0.5ex] 
 \thickhline
 10 & 0.01 \\
 10 & 0.01 \\
 10 & 0.01 \\
 10 & 0.01 \\
 10 & 0.01 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da $\mu$-learn nell'esperimento \texttt{E3}.}
\label{models_exp3}
\end{table}
\paragraph{E3 - Confronto con $\bm{\lambda}$}
Al fine di visualizzare il comportamento di $\lambda$, viene mostrato in Figura~\ref{prediction_exp3} il confronto tra le predizioni generate da tale predittore e la ground truth.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/prediction_memberships.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le predizioni effettuate da $\lambda$ nell'esperimento \texttt{E3}.}
    \label{prediction_exp3}
\end{figure} 

In Figura~\ref{rf_exp3} è possibile osservare il comportamento dei modelli ottenuti tramite Random Forest; essi vengono elencati in Tabella~\ref{models_rf_exp3}.

\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|c|c|} 
 \hline
\textbf{max profondità} & \textbf{max feature} & \textbf{min foglie campione} & \textbf{min partizioni campione} & \textbf{numero stimatori}
\\ [0.5ex] 
 \thickhline
 80 & 3 & 3 & 8 & 1000 \\ 
 80 & 3 & 3 & 8 & 1000 \\ 
 80 & 3 & 3 & 8 & 1000 \\ 
 80 & 3 & 3 & 10 & 300 \\ 
 80 & 3 & 3 & 8 & 1000 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da Random Forest nell'esperimento \texttt{E3}.}
\label{models_rf_exp3}
\end{table}

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/prediction_regression_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la regressione effettuata dall'algoritmo Random Forest nell'esperimento \texttt{E3}.}
    \label{rf_exp3}
\end{figure} 
I punteggi medi di RMSE per il training e il test error sono riportati in Tabella~\ref{rmse_exp3}.

\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \textbf{Training error} & \textbf{Test error}
\\ [0.5ex] 
 \thickhline
 $\lambda$ & 0.14 & 0.14 \\
 regressore random forest & \textbf{0.01} & \textbf{0.02}
 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{valori medi di RMSE per il predittore $\lambda$ e Random Forest nell'esperimento \texttt{E3}.}
\label{rmse_exp3}
\end{table}

\paragraph{E3 - Confronto con $\bm{\omega}$}
Nel caso del classificatore $\omega$ si osserva in Figura~\ref{classification_exp3} il confronto con le etichette attese. 
In Figura~\ref{4cases_exp3} vengono mostrati i grafici relativi ai veri negativi, veri positivi, falsi negativi e falsi positivi.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/prediction_classification.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le classificazioni effettuate da $\omega$ nell'esperimento \texttt{E3}.}
    \label{classification_exp3}
\end{figure}
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/tn.png}\label{tn_b05d}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/tp.png}\label{tp_b05d}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/fn.png}\label{fn_b05d}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/fp.png}\label{fp_b05d}
   \end{minipage}
   \caption{Gradi di appartenenza predetti da $\lambda$ e usati da $\omega$ per effettuare la classificazione. In alto a sinistra i veri negativi, a destra i veri positivi; in basso a sinistra i falsi negativi e a destra i falsi positivi nell'esperimento \texttt{E3}.}
   \label{4cases_exp3}
\end{figure}
In Figura~\ref{rf_class_exp3} vengono visualizzate le predizioni di Random Forest su questo tipo di dataset.

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_disgiunti/prediction_classification_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la classificazione effettuata da Random Forest nell'esperimento \texttt{E3}.}
    \label{rf_class_exp3}
\end{figure} 
Si chiude questa prima parte di esperimenti con la Tabella~\ref{prf_exp3}, relativa ai punteggi medi di Precision, Recall e F1.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \begin{tabular}{@{}c@{}}\textbf{Training} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular} & \begin{tabular}{@{}c@{}}\textbf{Test} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular}
\\ [0.5ex] 
 \thickhline
 $\omega$ & 0.62, 0.77, 0.69 & 0.63, 0.77, 0.69 \\
 classificatore random forest & \textbf{1.0, 0.99, 1.0} & \textbf{0.99, 0.99, 0.99} \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Valori medi di Precision, Recall e F1 per il predittore $\omega$ e la baseline nell'esperimento \texttt{E3}.}
\label{prf_exp3}
\end{table}
L'algoritmo Random Forest si rivela particolarmente efficace anche in questo caso, riuscendo a riprodurre in modo molto accurato la distribuzione di partenza e compiendo un errore davvero contenuto.
I modelli generati da $\mu$-learn rivelano dei punteggi accettabili sia nel caso della classificazione che dell'induzione del grado di appartenenza, tuttavia nel confronto con la baseline tali punteggi risultano fin qui inferiori.


\subsubsection{Insiemi sovrapposti}\label{insiemisovrapposti}
L'altra sequenza di esperimenti si basa su insiemi di parole di partenza con una sovrapposizione del $17\%$. Anche in questo caso gli esperimenti si articolano nelle tre distribuzioni utilizzate in precedenza. Lo scopo di questa parte di esperimenti è vedere come variano i risultati.

\paragraph{E4 - Distribuzione uniforme} Analogamente all'esperimento \texttt{E1} in cui si considera il dataset ricavato da una distribuzione uniforme continua per gli insiemi disgiunti (Paragrafo~\ref{insiemidisgiunti}), si utilizza in questo esperimento il dataset ottenuto dalla stessa distribuzione per il caso di insiemi parzialmente sovrapposti. In Tabella~\ref{models_exp4} vengono menzionati quali modelli siano stati reputati ottimali per questo tipo di dati.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|} 
 \hline
$\bm{\sigma}$ & \textit{\textbf{C}}
\\ [0.5ex] 
 \thickhline
 1000 & 100 \\
 1000 & 100 \\
 1000 & 10 \\
 1000 & 100 \\
 1000 & 100 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da $\mu$-learn nell'esperimento \texttt{E4}.}
\label{models_exp4}
\end{table}
\paragraph{E4 - Confronto con $\bm{\lambda}$}
In Figura~\ref{prediction_exp4} è raffigurato il comportamento di $\lambda$ nel generare le predizioni e il risultato è pressoché identico a quello ritrovato nel caso di insiemi disgiunti.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/prediction_memberships.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le predizioni effettuate da $\lambda$ nell'esperimento \texttt{E4}.}
    \label{prediction_exp4}
\end{figure} 
Le predizioni dell'algoritmo Random Forest vengono mostrate in Figura~\ref{rf_exp4} e sono relative ai modelli riportati in Tabella~\ref{models_rf_exp4}.

\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|c|c|} 
 \hline
\textbf{max profondità} & \textbf{max feature} & \textbf{min foglie campione} & \textbf{min partizioni campione} & \textbf{numero stimatori}
\\ [0.5ex] 
 \thickhline
 80 & 3 & 3 & 8 & 100 \\ 
 80 & 3 & 3 & 8 & 100 \\ 
 80 & 3 & 4 & 10 & 100 \\ 
 80 & 3 & 3 & 8 & 100 \\ 
 80 & 3 & 3 & 8 & 100 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da Random Forest nell'esperimento \texttt{E4}.}
\label{models_rf_exp4}
\end{table}

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/prediction_regression_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la regressione effettuata dall'algoritmo Random Forest nell'esperimento \texttt{E4}.}
    \label{rf_exp4}
\end{figure} 

Successivamente, si mostrano in Tabella~\ref{rmse_exp4} i punteggi medi di RMSE per il training e il test error. 
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \textbf{Training error} & \textbf{Test error}
\\ [0.5ex] 
 \thickhline
 $\lambda$ & 0.20 & 0.21 \\
 regressore random forest & \textbf{0.02} & \textbf{0.03}
 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{valori medi di RMSE per il predittore $\lambda$ e Random Forest nell'esperimento \texttt{E4}.}
\label{rmse_exp4}
\end{table}

\paragraph{E4 - Confronto con $\bm{\omega}$}
In Figura~\ref{classification_exp4} si passa nuovamente al caso del classificatore $\omega$ per insiemi sovrapposti, approfondendo nel dettaglio in Figura~\ref{4cases_exp4} i grafici relativi alle quattro situazioni descritte dalla matrice di confusione.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/prediction_classification.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le classificazioni effettuate da $\omega$ nell'esperimento \texttt{E4}.}
    \label{classification_exp4}
\end{figure}
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/tn.png}\label{tn_us}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/tp.png}\label{tp_us}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/fn.png}\label{fn_us}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/fp.png}\label{fp_us}
   \end{minipage}
   \caption{Gradi di appartenenza predetti da $\lambda$ e usati da $\omega$ per effettuare la classificazione. In alto a sinistra i veri negativi, a destra i veri positivi; in basso a sinistra i falsi negativi e a destra i falsi positivi nell'esperimento \texttt{E4}.}
   \label{4cases_exp4}
\end{figure}
Per quanto riguarda le previsioni dell'algoritmo Random Forest, in Figura~\ref{rf_class_exp4} viene riportato il confronto con la ground truth.

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_uniform_sovrapposti/prediction_classification_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la classificazione effettuata da Random Forest nell'esperimento \texttt{E4}.}
    \label{rf_class_exp4}
\end{figure} 
Di conseguenza, si mostrano i relativi punteggi medi di Precision, Recall e F1 in Tabella~\ref{prf_exp4}.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \begin{tabular}{@{}c@{}}\textbf{Training} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular} & \begin{tabular}{@{}c@{}}\textbf{Test} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular}
\\ [0.5ex] 
 \thickhline
 $\omega$ & 0.73, 0.91, 0.81 & 0.75, 0.86, 0.80 \\
 classificatore random forest & \textbf{0.99, 0.99, 0.99} & \textbf{0.98, 0.98, 0.98} \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Valori medi di Precision, Recall e F1 per il predittore $\omega$ e la baseline nell'esperimento \texttt{E4}.}
\label{prf_exp4}
\end{table}

\paragraph{E5 - Distribuzione Beta con $\bm{\alpha=\beta=5}$}
Nel quinto esperimento si considera il caso di un dataset generato da una distribuzione Beta con parametri $\alpha=\beta=5$ a partire da insiemi parzialmente sovrapposti; i modelli scelti tramite la grid search vengono riportati nella Tabella~\ref{models_exp5}.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|} 
 \hline
$\bm{\sigma}$ & \textit{\textbf{C}}
\\ [0.5ex] 
 \thickhline
 1000 & 1 \\
 1000 & 1 \\
 1000 & 1 \\
 1000 & 1 \\
 1000 & 1 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da $\mu$-learn nell'esperimento \texttt{E5}.}
\label{models_exp5}
\end{table}
\paragraph{E5 - Confronto con $\bm{\lambda}$}
In Figura~\ref{prediction_exp5} si visualizza i grafici della ground truth dei dati generati e dei gradi di appartenenza trovati dai modelli con il predittore $\lambda$.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/prediction_memberships.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le predizioni effettuate da $\lambda$ nell'esperimento \texttt{E5}.}
    \label{prediction_exp5}
\end{figure} 
La grid search è stata applicata anche all'algoritmo Random Forest, generando i modelli riassunti in Tabella~\ref{models_rf_exp5}; tali modelli hanno prodotto le previsioni mostrate in Figura~\ref{rf_exp5}.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|c|c|} 
 \hline
\textbf{max profondità} & \textbf{max feature} & \textbf{min foglie campione} & \textbf{min partizioni campione} & \textbf{numero stimatori}
\\ [0.5ex] 
 \thickhline
 80 & 3 & 3 & 8 & 300 \\ 
 80 & 3 & 4 & 8 & 1000 \\ 
 80 & 3 & 3 & 8 & 300 \\ 
 80 & 3 & 3 & 8 & 100 \\ 
 80 & 3 & 3 & 8 & 1000 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da Random Forest nell'esperimento \texttt{E5}.}
\label{models_rf_exp5}
\end{table}

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/prediction_regression_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la regressione effettuata dall'algoritmo Random Forest nell'esperimento \texttt{E5}.}
    \label{rf_exp5}
\end{figure} 

Sono stati, dunque, calcolati i punteggi medi di RMSE per il training e il test error (Tabella~\ref{rmse_exp5}). 
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \textbf{Training error} & \textbf{Test error}
\\ [0.5ex] 
 \thickhline
 $\lambda$ & 0.13 & 0.13 \\
 regressore random forest & \textbf{0.02} & \textbf{0.03}
 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{valori medi di RMSE per il predittore $\lambda$ e Random Forest nell'esperimento \texttt{E5}.}
\label{rmse_exp5}
\end{table}

\paragraph{E5 - Confronto con $\bm{\omega}$}
Nel caso della classificazione, si propone in Figura~\ref{classification_exp5} il confronto tra le etichette attese e le previsioni di $\omega$. 
In Figura~\ref{4cases_exp5} vengono mostrati i grafici inerenti alle quattro categorie presenti nella matrice di confusione.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/prediction_classification.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le classificazioni effettuate da $\omega$ nell'esperimento \texttt{E5}.}
    \label{classification_exp5}
\end{figure}
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/tn.png}\label{tn_b5s}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/tp.png}\label{tp_b5s}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/fn.png}\label{fn_b5s}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/fp.png}\label{fp_b5s}
   \end{minipage}
   \caption{Gradi di appartenenza predetti da $\lambda$ e usati da $\omega$ per effettuare la classificazione. In alto a sinistra i veri negativi, a destra i veri positivi; in basso a sinistra i falsi negativi e a destra i falsi positivi nell'esperimento \texttt{E5}.}
   \label{4cases_exp5}
\end{figure}
In Figura~\ref{rf_class_exp5} viene riportato il confronto tra le etichette attese e il classificatore Random Forest in arancione.

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta5_sovrapposti/prediction_classification_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la classificazione effettuata da Random Forest nell'esperimento \texttt{E5}.}
    \label{rf_class_exp5}
\end{figure} 
In Tabella~\ref{prf_exp5} vengono riportati i punteggi medi di Precision, Recall e F1.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \begin{tabular}{@{}c@{}}\textbf{Training} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular} & \begin{tabular}{@{}c@{}}\textbf{Test} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular}
\\ [0.5ex] 
 \thickhline
 $\omega$ & 0.60, 0.81, 0.69 & 0.61, 0.78, 0.68 \\
 classificatore random forest & \textbf{0.96, 0.97, 0.97} & \textbf{0.94, 0.95, 0.95} \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Valori medi di Precision, Recall e F1 per il predittore $\omega$ e la baseline nell'esperimento \texttt{E5}.}
\label{prf_exp5}
\end{table}

\paragraph{E6 - Distribuzione Beta con $\bm{\alpha=\beta=0.5}$}
Nel sesto esperimento è stato utilizzato il dataset generato dalla distribuzione Beta con parametri $\alpha=\beta=0.5$ a partire da insiemi parzialmente sovrapposti; i modelli scelti sono annotati nella Tabella~\ref{models_exp6}.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|} 
 \hline
 $\bm{\sigma}$ & \textit{\textbf{C}}
\\ [0.5ex] 
 \thickhline
 10 & 0.01 \\
 10 & 0.01 \\
 1000 & 0.01 \\
 10 & 0.01 \\
 10 & 0.01 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da $\mu$-learn nell'esperimento \texttt{E6}.}
\label{models_exp6}
\end{table}
\paragraph{E6 - Confronto con $\bm{\lambda}$}
In Figura~\ref{prediction_exp6} sono visualizzate graficamente le predizioni generate dal predittore $\lambda$ e la ground truth.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/prediction_memberships.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le predizioni effettuate da $\lambda$ nell'esperimento \texttt{E6}.}
    \label{prediction_exp6}
\end{figure} 

In Figura~\ref{rf_exp6} è possibile osservare il comportamento dei modelli ottenuti tramite Random Forest; essi vengono elencati in Tabella~\ref{models_rf_exp6}.

\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|c|c|} 
 \hline
\textbf{max profondità} & \textbf{max feature} & \textbf{min foglie campione} & \textbf{min partizioni campione} & \textbf{numero stimatori}
\\ [0.5ex] 
 \thickhline
 80 & 3 & 3 & 8 & 100 \\ 
 80 & 3 & 3 & 8 & 1000 \\ 
 80 & 2 & 3 & 8 & 100 \\ 
 80 & 3 & 3 & 8 & 300 \\ 
 80 & 3 & 3 & 8 & 100 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da Random Forest nell'esperimento \texttt{E6}.}
\label{models_rf_exp6}
\end{table}

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/prediction_regression_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la regressione effettuata dall'algoritmo Random Forest nell'esperimento \texttt{E6}.}
    \label{rf_exp6}
\end{figure} 

Anche in questo caso i punteggi medi di RMSE per il training e il test error sono stati calcolati e vengono mostrati in Tabella~\ref{rmse_exp6}. 
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \textbf{Training error} & \textbf{Test error}
\\ [0.5ex] 
 \thickhline
 $\lambda$ & 0.19 & 0.19 \\
 regressore random forest & \textbf{0.02} & \textbf{0.03}
 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{valori medi di RMSE per il predittore $\lambda$ e Random Forest nell'esperimento \texttt{E6}.}
\label{rmse_exp6}
\end{table}

\paragraph{E6 - Confronto con $\bm{\omega}$}
Nel caso del classificatore $\omega$ si osserva in Figura~\ref{classification_exp6} il confronto con le etichette attese. 
In Figura~\ref{4cases_exp6} vengono mostrati i grafici relativi ai veri negativi, veri positivi, falsi negativi e falsi positivi.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/prediction_classification.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le classificazioni effettuate da $\omega$ nell'esperimento \texttt{E6}.}
    \label{classification_exp6}
\end{figure}
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/tn.png}\label{tn_b05s}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/tp.png}\label{tp_b05s}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/fn.png}\label{fn_b05s}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/fp.png}\label{fp_b05s}
   \end{minipage}
   \caption{Gradi di appartenenza predetti da $\lambda$ e usati da $\omega$ per effettuare la classificazione. In alto a sinistra i veri negativi, a destra i veri positivi; in basso a sinistra i falsi negativi e a destra i falsi positivi nell'esperimento \texttt{E6}.}
   \label{4cases_exp6}
\end{figure}
In Figura~\ref{rf_class_exp6} vengono visualizzate le predizioni di Random Forest su questo tipo di distribuzione.

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_beta05_sovrapposti/prediction_classification_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la classificazione effettuata da Random Forest nell'esperimento \texttt{E6}.}
    \label{rf_class_exp6}
\end{figure} 
In conclusione, in Tabella~\ref{prf_exp6} vengono riassunti i punteggi medi di Precision, Recall e F1.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \begin{tabular}{@{}c@{}}\textbf{Training} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular} & \begin{tabular}{@{}c@{}}\textbf{Test} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular}
\\ [0.5ex] 
 \thickhline
 $\omega$ & 0.82, 0.98, 0.89 & 0.83, 0.97, 0.89 \\
 classificatore random forest & \textbf{1.0, 0.99, 0.99} & \textbf{0.99, 0.99, 0.99} \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Valori medi di Precision, Recall e F1 per il predittore $\omega$ e la baseline nell'esperimento \texttt{E6}.}
\label{prf_exp6}
\end{table}
La serie di esperimenti relativa agli insiemi sovrapposti non influenza in maniera così determinante i risultati.
È possibile che questo sia dovuto alla bassa percentuale di sovrapposizione prevista tra i due insiemi: in questo caso si è preferito fornire un punto di partenza contenuto ma, per eventuali sviluppi futuri, si potrebbe prendere in considerazione l'idea di usare percentuali più alte. 
In generale, la lunga sequenza di esperimenti relativa ai dati generati non ha prodotto risultati a favore del sistema basato sull'approccio fuzzy; al contrario, apparentemente risulta che un algoritmo come Random Forest possa essere sfruttato per generare predizioni attendibili.
In realtà, gli esperimenti finora considerati trattano dataset di natura artificiale, generati tramite un processo stocastico. Tali dataset, inoltre, non presentano ragionamenti, opinioni o costruzioni di frasi con una semantica ma sono stati pensati per catturare una distinzione che formalmente esiste solo da un punto di vista sintattico.
L'assunzione fondamentale fatta in questa tesi è che il problema delle fake news sia modellabile secondo la logica fuzzy, basandosi sul fatto che questo tipo di notizie, in realtà, presenti una componente di incertezza intrinsecamente complessa che dipende dalla sintassi, dalla semantica e dall'ambiguità del linguaggio umano e che oggi rappresentano il motivo per cui non risulta semplice rilevarle automaticamente.
Alla luce delle considerazioni sin qui fatte, si pongono le basi per analizzare l'ultimo esperimento condotto, relativo ai dati reali compresi nel dataset presentato nel Paragrafo~\ref{fakenewsdataset}.

\subsection{Esperimento con il dataset \textit{Fake and real news}}\label{kaggleexperiment}
Questo esperimento coinvolge il dataset descritto nel Paragrafo~\ref{fakenewsdataset}, presente sulla piattaforma Kaggle. In questo tipo di esperimento l'apprendimento è avvenuto tramite delle etichette binarie e riguarda delle notizie che sono state raccolte mediante Web Scraping.
Al fine di fornire un confronto con alcuni stimatori allo stato dell'arte, è stata formata una baseline di predittori in parte destinati alla classificazione e, dunque, a un paragone con $\omega$, in parte dedicati alla regressione per $\lambda$.
Per quanto riguarda l'algoritmo $\mu$-learn, in Tabella~\ref{models_exp7} vengono mostrate le configurazioni dei modelli selezionati durante la model selection e, a partire da essi, sono stati ricavati $\lambda$ e $\omega$.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|} 
 \hline
$\bm{\sigma}$ & \textit{\textbf{C}}
\\ [0.5ex] 
 \thickhline
 1000 & 100 \\
 100000 & 10 \\
 1000 & 100 \\
 100000 & 100 \\
 100000 & 100 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da $\mu$-learn nell'esperimento \texttt{E7}.}
\label{models_exp7}
\end{table}
\paragraph{E7 - Confronto con $\bm{\lambda}$}
In Figura~\ref{prediction_exp7} viene riportato un confronto tra la ground truth e i gradi di appartenenza predetti dai modelli con il predittore $\lambda$.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/expected_memberships.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/prediction_memberships.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le predizioni effettuate da $\lambda$ nell'esperimento \texttt{E7}.}
    \label{prediction_exp7}
\end{figure} 
In questo caso si osserva la presenza di tre componenti: due in corrispondenza dei valori estremi a indicare notizie predette come completamente fake e completamente non fake, e una terza componente di valori che si distribuiscono uniformemente nell'intervallo $[0,1]$.

In Figura~\ref{rf_exp7} vengono mostrate le predizioni dell'algoritmo Random Forest, per cui è stata eseguita la grid search per la ricerca dei parametri ottimali; i valori di tali parametri sono riportati in Tabella~\ref{models_rf_exp7}.

\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|c|c|} 
 \hline
\textbf{max profondità} & \textbf{max feature} & \textbf{min foglie campione} & \textbf{min partizioni campione} & \textbf{numero stimatori}
\\ [0.5ex] 
 \thickhline
 80 & 3 & 3 & 8 & 100 \\ 
 80 & 3 & 4 & 8 & 1000 \\ 
 80 & 3 & 4 & 8 & 1000 \\ 
 80 & 3 & 3 & 10 & 100 \\ 
 80 & 3 & 3 & 8 & 1000 \\ 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Modelli migliori generati da Random Forest nell'esperimento \texttt{E7}.}
\label{models_rf_exp7}
\end{table}

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/prediction_regression_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la regressione effettuata dall'algoritmo Random Forest nell'esperimento \texttt{E7}.}
    \label{rf_exp7}
\end{figure} 
In questo caso, l'andamento dei valori è molto diverso rispetto a quello ricavato da $\mu$-learn: il risultato è una distribuzione piuttosto incerta il cui picco sembra essere influenzato dalla presenza di più occorrenze per il valore 0.

Sono stati, dunque, calcolati i punteggi medi di RMSE per il training e il test error (Tabella~\ref{rmse_exp7}). Si specifica che, per via della natura del dataset, per entrambi i modelli persiste una buona componente d'errore dovuta alla forma binaria delle etichette. Da questo punto di vista gli errori dei modelli sul test set si equivalgono, tuttavia, visivamente, mostrano un approccio diverso nel modellare l'andamento dei gradi di appartenenza.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \textbf{Training error} & \textbf{Test error}
\\ [0.5ex] 
 \thickhline
 $\lambda$ & 0.30 & 0.37 \\
 regressore random forest & \textbf{0.25} & 0.37
 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{valori medi di RMSE per il predittore $\lambda$ e Random Forest nell'esperimento \texttt{E7}.}
\label{rmse_exp7}
\end{table}

\paragraph{E7 - Confronto con $\bm{\omega}$}
La Figura~\ref{classification_exp7} illustra un confronto analogo a quello del paragrafo precedente, considerando ora il classificatore $\omega$.
In Figura~\ref{4cases_exp7} vengono mostrati i grafici relativi alle quattro casistiche della matrice di confusione.
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/prediction_classification.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le classificazioni effettuate da $\omega$ nell'esperimento \texttt{E7}.}
    \label{classification_exp7}
\end{figure}
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_kaggle/tn.png}\label{tn_k}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_kaggle/tp.png}\label{tp_k}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_kaggle/fn.png}\label{fn_k}
   \end{minipage}
   \begin{minipage}{0.48\textwidth}
     \includegraphics[width=\linewidth]{images/experiment_kaggle/fp.png}\label{fp_k}
   \end{minipage}
   \caption{Gradi di appartenenza predetti da $\lambda$ e usati da $\omega$ per effettuare la classificazione. In alto a sinistra i veri negativi, a destra i veri positivi; in basso a sinistra i falsi negativi e a destra i falsi positivi nell'esperimento \texttt{E7}.}
   \label{4cases_exp7}
\end{figure}
Tali grafici confermano la presenza di una componente di incertezza: i falsi negativi e i falsi positivi mostrano una distribuzione ben più omogenea rispetto ai veri negativi e ai veri positivi e questo dimostra che il sistema proposto in questa tesi è più drastico quando predice correttamente l'etichetta mentre si mostra più indeciso quando commette un errore. Nelle Figure~\ref{rf_class_exp7}-\ref{svc_exp7} vengono riportati i confronti tra le etichette attese e i classificatori della baseline: in ordine un classificatore Random Forest (in arancione), un albero di decisione (in verde), una rete neurale (in giallo) e un classificatore support vector (in rosso).

\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/prediction_classification_rf.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra la classificazione effettuata da Random Forest nell'esperimento \texttt{E7}.}
    \label{rf_class_exp7}
\end{figure} 
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/prediction_classification_dectree.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le classificazioni effettuate dall'albero di decisione nell'esperimento \texttt{E7}.}
    \label{dt_exp7}
\end{figure} 
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/prediction_classification_nn.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le classificazioni effettuate dalla rete neurale nell'esperimento \texttt{E7}.}
    \label{nn_exp7}
\end{figure} 
\begin{figure}
\centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/expected_classification.png}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\linewidth]{images/experiment_kaggle/prediction_classification_svc.png}
    \end{minipage}
    \caption{A sinistra le etichette attese, a destra le classificazioni effettuate dal classificatore support vector nell'esperimento \texttt{E7}.}
    \label{svc_exp7}
\end{figure} 
In Tabella~\ref{prf_exp7} vengono riportati i punteggi medi di Precision, Recall e F1 per $\omega$ e per la baseline.
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
 \begin{tabular}{|c|c|c|} 
 \hline
\textbf{Predittore} & \begin{tabular}{@{}c@{}}\textbf{Training} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular} & \begin{tabular}{@{}c@{}}\textbf{Test} \\ \textbf{Precision}, \textbf{Recall}, \textbf{F1} \end{tabular}
\\ [0.5ex] 
 \thickhline
 $\omega$ & 0.82, 0.97, 0.89 & 0.78, 0.89, 0.83 \\
 classificatore random forest & 0.99, 0.98, 0.98 & 0.87, 0.72, 0.79 \\
 albero di decisione & \textbf{1.0, 1.0, 1.0} & 0.70, 0.66, 0.68 \\
 rete neurale & 0.93, 0.92, 0.93 & 0.86, 0.86, 0.86 \\
 classificatore support vector & 0.93, 0.91, 0.92 & \textbf{0.90, 0.86, 0.88} \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Valori medi di Precision, Recall e F1 per il predittore $\omega$ e la baseline nell'esperimento \texttt{E7}.}
\label{prf_exp7}
\end{table}
Nel caso della classificazione, modelli basati su support vector si rivelano superiori nel produrre predizioni accurate.
In conclusione, gli esperimenti hanno evidenziato i seguenti fatti:
\begin{itemize}
    \item l'algoritmo $\mu$-learn può rappresentare un'alternativa ai modelli per la classificazione ma, oltre a non essere il più efficace, non si tratta della sua funzione originaria;
    \item se si generano misture di parole appartenenti a topic differenti tramite un processo stocastico, come la Latent Dirichlet Allocation, l'algoritmo Random Forest ottiene notevoli risultati, riproducendo in modo molto accurato le etichette attese;
    \item la componente semantica di un documento testuale influisce pesantemente sul riconoscimento;
    \item l'algoritmo $\mu$-learn potrebbe avere la potenzialità di individuare una componente di incertezza delle notizie ad appartenere all'insieme delle fake news.
\end{itemize}

\subsubsection{Componente di incertezza}
Gli esperimenti del Paragrafo~\ref{kaggleexperiment} hanno evidenziato la presenza di una componente di incertezza nelle predizioni effettuate dall'algoritmo $\mu$-learn, utilizzato nel sistema proposto in questa tesi. 

In Figura~\ref{overlaypred} viene mostrata una visualizzazione alternativa delle predizioni che permette di distinguere queste ultime in fake e non fake secondo la ground truth e, soprattutto, mostra più esplicitamente tale componente di incertezza.
\begin{figure}
    \centering
    \includegraphics[scale=0.48]{images/experiment_kaggle/overlaypred.png}
    \caption{Predizioni dell'algoritmo $\mu$-learn. In rosso le predizioni etichettate come fake, in azzurro quelle vere.}
    \label{overlaypred}
\end{figure}
In corrispondenza dei valori estremi, laddove è presente la sovrapposizione, si parla rispettivamente di falsi negativi e falsi positivi. Nella parte centrale del grafico, invece, si nota una fascia progressivamente mista delle due componenti, evidenziando un aspetto di incertezza da parte del predittore ad assegnare dei gradi di appartenenza a un tipo di notizia piuttosto che all'altro.
A tal proposito, un possibile approccio consiste nel definire un raggio $r$ e una soglia $s$, al fine di individuare un intervallo di incertezza $[s-r,s+r]$.
\begin{figure}
    \centering
    \includegraphics[scale=0.48]{images/experiment_kaggle/indecisionrf.png}
    \caption{Predizioni dell'algoritmo Random Forest. In rosso le predizioni etichettate come fake, in azzurro quelle vere.}
    \label{indecisionrf}
\end{figure}
In Figura~\ref{indecisionrf} lo stesso tipo di visualizzazione viene fatta per le predizioni ottenute tramite l'algoritmo Random Forest. Anche qui si distinguono tre componenti, tuttavia l'intersezione è più ampia e si osserva un comportamento meno drastico da parte del regressore.
L'esito dell'esperimento, inoltre, suggerisce che, qualora l'assunzione che il problema di classificazione delle fake news fosse di natura fuzzy, possa essere interessante considerare un problema di classificazione multi-classe, e in particolare delle etichette a valori ternari. Questo significa che, oltre a `fake'' e ``non fake'', si aggiungerebbe una terza classe di notizie la cui natura è più incerta e la cui affidabilità potrebbe dover richiedere maggior attenzione da parte dell'utente durante la loro lettura.

\chapter*{Conclusioni e sviluppi futuri}
\addcontentsline{toc}{chapter}{Conclusioni e sviluppi futuri}
\markboth{Conclusioni e sviluppi futuri}{} 
\onehalfspacing
In questa tesi è stato proposto un sistema di rilevazione delle fake news basato sull'individuazione dell'insieme fuzzy associato, tramite il quale è stato possibile produrre dei punteggi di \textit{fakeness} delle notizie basandosi sul grado di appartenenza a tale insieme.
Il sistema è comprensivo delle tipiche fasi che contraddistinguono la strategia di apprendimento automatico supervisionato nell'ambito dell'analisi testuale, includendo una parte di rielaborazione del testo, la generazione di modelli predittivi con configurazione ottimale degli iperparametri e la valutazione finale dei risultati.

Il sistema è stato testato su due tipi di dataset: alcuni sono stati generati artificialmente tramite la tecnica LDA, in cui le variabili target sono valori continui in $[0,1]$, oltre a un dataset aggiuntivo, scaricato da Kaggle, in cui sono presenti delle etichette binarie.
Nel lavoro svolto in questa tesi sono stati proposti, per entrambi i tipi di dataset, due task: la classificazione binaria e l'induzione del grado di appartenenza. Dai risultati ottenuti negli esperimenti è emerso che il sistema non eccelle particolarmente nelle predizioni dei dati sintetici ma, al contrario, esprime il suo potenziale sulle notizie raccolte dal Web. In particolare, i gradi di appartenenza prodotti su queste ultime mostrano un andamento riconducibile a una classificazione binaria abbastanza efficace, con l'aggiunta di una terza componente di incertezza che si ritiene il caso di investigare ulteriormente in futuro.
In generale, l'intervallo di indagine considerato per gli esperimenti è ampio e, in questo senso, si auspica che la tesi possa rappresentare un punto di partenza per sviluppi futuri; in tal senso, segue un elenco di alcune strade percorribili:
\begin{itemize}
    \item ripetere gli esperimenti con un campione di dati più grande;
    \item integrare l'analisi delle fake news con tecniche di image e video processing per modellare il problema più complesso in cui anche immagini e video svolgono un ruolo centrale nelle fake news; ad esempio immagini vecchie riusate per notizie nuove o video che non corrispondono ai fatti riportati;
    \item considerare non solo il corpo delle notizie ma anche il titolo;
    \item considerare tecniche più sofisticate come le reti neurali convoluzionali per generare testi per l'addestramento del sistema;
    \item variare la grandezza dell'intersezione tra i topic durante il processo di generazione dei documenti testuali;
    \item ricavare un dataset di notizie reali con un grado di appartenenza associato;
    \item testare nuove grid search che coinvolgano diversi tipi di fuzzificatori e di kernel;
    \item provare diverse combinazioni della pipeline di preprocessing;
    \item osservare come variano i risultati con nuove tecniche di embedding di recente sviluppo, ad esempio GloVe, Elmo e BERT.
\end{itemize}

Si ritiene, quindi, che gli obiettivi iniziali della tesi siano stati raggiunti, ottenendo un sistema in grado di produrre per le notizie i punteggi di \textit{fakeness} desiderati.
Infine, il fatto che gli sviluppi possibili sono molteplici induce a pensare che, potenzialmente, il sistema proposto abbia ampi margini di miglioramento.

\printbibliography

%			RINGRAZIAMENTI
%
\prefacesection{Ringraziamenti}
Nonostante io faccia ancora fatica a crederci, mi rendo conto solo adesso di essere giunto alla fine del mio percorso universitario. Si conclude il capitolo più importante della mia vita, quello che mi ha consentito maggiormente di crescere, imparare, maturare e formare la persona che sono oggi.
Non posso che ringraziare i miei storici compagni di studio, tramutati poi in solidi amici: Punta, Alex, Bobe, Fil, Carmine, J, nonostante la maggior parte di loro abbia preso una strada diversa dopo la laurea triennale, il merito va anche a loro.
Ringrazio le splendide persone che ho incontrato durante l'Erasmus: Noemi, Egon, Leo, Roberto, che mi hanno accompagnato in una delle esperienze più entusiasmanti che ho potuto vivere prima della pandemia.
Ringrazio i miei amici fuori dal contesto universitario, fondamentali per offrirmi nuove lezioni da imparare e per aver riposto fiducia nella mia amicizia: Della, Samu, Roch, Jaco, Tenu, Ale, e fortunatamente, la lista è ancora lunga.
Ringrazio il Professor Malchiodi e il Professor Ferrara per avermi guidato in questo stimolante lavoro durante un periodo storico così complicato e che, paradossalmente, mi ha dato l'ispirazione per analizzare il tema delle fake news.
Infine, voglio ringraziare la mia famiglia, assolutamente determinante per il supporto che mi danno e per essere sempre i primi a credere in me.
Mi rendo conto che nominare tutti quanti non sia possibile e allora vorrei concludere i miei ringraziamenti con una metafora. Durante il lavoro di questa tesi ho avuto modo di conoscere l'esistenza della Legge di Zipf che, tra i vari campi di applicazione, descrive una curiosa caratteristica di tutti i testi di tutte le lingue oggi conosciute: 
poche parole compaiono spesso mentre la maggior parte dei termini appare raramente. Tali termini sono, quindi, ugualmente fondamentali per formare il senso delle frasi e, dunque, anche chi non ho nominato, in realtà, ha avuto un significato in questa mia esperienza. Ringrazio tutti quanti per avermi permesso di sancire la fine di questo percorso e di far spazio a un nuovo inizio.


\end{document}



